{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:**\n",
    "\n",
    "Fit a logistic regression model to the mushrooms dataset from the UCI data repository using 10-fold cross-validation. Next, fit a (Bernoulli) Naïve Bayes model to the same dataset using the same criteria.  First, report the average accuracy scores for each type of \n",
    "\n",
    "Next, generate aggregate contingency tables for each type of classifier. \n",
    "\n",
    "Which type of classifier would you use when trying to decide if you want to eat a mushroom? Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8124, 23)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "mush_features = [\"Edibility\",\"Capshape\", \"Capsurface\",\"Capcolor\",\"Bruises\",\"Odor\",\"Gill_attachment\",\"Gill_space\",\"Gill_size\",\"Gill_color\",\"Stalk_shape\",\"Stalk_root\",\"Stalk_surface_br\",\"Stalk_surface_ar\",\"Stalk_color_ar\",\"Stalk_color_br\",\"Veil_type\",\"Veil_color\",\"Ring_no\",\"Ring_type\",\"Spore_color\",\"Pop\",\"Habitat\"]\n",
    "df_mushroom = pd.read_csv(\"agaricus-lepiota.data.txt\",header = None, names = mush_features)\n",
    "df_mushroom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[0 0 1 ..., 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nivea\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.utils import shuffle\n",
    " \n",
    "mush_X = []\n",
    "\n",
    "targetcols = ['Edibility']\n",
    "datacols= ['Capshape', 'Capsurface','Capcolor','Bruises','Odor','Gill_attachment','Gill_space','Gill_size','Gill_color','Stalk_shape','Stalk_root','Stalk_surface_br','Stalk_surface_ar','Stalk_color_ar','Stalk_color_br','Veil_type','Veil_color','Ring_no','Ring_type','Spore_color','Pop','Habitat']\n",
    "mush_shuffled = shuffle(df_mushroom)\n",
    "mushroom_X = mush_shuffled[datacols].values\n",
    "#print(mushroom_X.shape)\n",
    "mushroom_Y = mush_shuffled[targetcols].values\n",
    "#print(mushroom_Y.shape)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "le = LabelEncoder()\n",
    "#print (mushroom_X.shape[1])\n",
    "for i in range(mushroom_X.shape[1]):\n",
    "    mushroom_X[:,i] = le.fit_transform(mushroom_X[:,i])\n",
    "#print(mushroom_X)\n",
    "onehotencoder = OneHotEncoder(categorical_features='all')\n",
    "mush_X = onehotencoder.fit_transform(mushroom_X).toarray()\n",
    "print(mush_X)\n",
    "\n",
    "mushroom_Y = le.fit_transform(mushroom_Y)\n",
    "print(mushroom_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression Accuracy\n",
      "1 Trial Accuracy:  0.945879458795\n",
      "2 Trial Accuracy:  0.947109471095\n",
      "3 Trial Accuracy:  0.944649446494\n",
      "4 Trial Accuracy:  0.952029520295\n",
      "5 Trial Accuracy:  0.950738916256\n",
      "6 Trial Accuracy:  0.953201970443\n",
      "7 Trial Accuracy:  0.948275862069\n",
      "8 Trial Accuracy:  0.955665024631\n",
      "9 Trial Accuracy:  0.958128078818\n",
      "10 Trial Accuracy:  0.948275862069\n",
      "\n",
      "\n",
      "Avg. Accuracy for Logistic Regression:  0.950395361096\n",
      "Confusion Matrix for Logistic Regression: \n",
      " [[ 4033.   175.]\n",
      " [  228.  3688.]]\n",
      "\n",
      " Bernoulli Naive Bayes Accuracy\n",
      "1 Trial Accuracy:  0.829028290283\n",
      "2 Trial Accuracy:  0.832718327183\n",
      "3 Trial Accuracy:  0.848708487085\n",
      "4 Trial Accuracy:  0.847478474785\n",
      "5 Trial Accuracy:  0.823891625616\n",
      "6 Trial Accuracy:  0.844827586207\n",
      "7 Trial Accuracy:  0.8460591133\n",
      "8 Trial Accuracy:  0.838669950739\n",
      "9 Trial Accuracy:  0.839901477833\n",
      "10 Trial Accuracy:  0.857142857143\n",
      "\n",
      "\n",
      "Avg. Accuracy for Bernoulli Naive Bayes:  0.895618990057\n",
      "Confusion Matrix for Bernoulli Naive Bayes: \n",
      " [[ 3942.   266.]\n",
      " [ 1027.  2889.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cv = cross_validation.KFold(len(mushroom_X), n_folds=10)\n",
    "acc_res = []\n",
    "cm_totallr= np.zeros((2,2))\n",
    "\n",
    "\n",
    "print(\"\\n Logistic Regression Accuracy\")\n",
    "for i, (train_mush, test_mush) in enumerate(cv):\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(mushroom_X[train_mush],mushroom_Y[train_mush])\n",
    "    y_predclf = clf.predict(mushroom_X[test_mush])\n",
    "    print(i+1 ,\"Trial Accuracy: \",accuracy_score(y_predclf, mushroom_Y[test_mush]))\n",
    "    acc_res.append(accuracy_score(y_predclf, mushroom_Y[test_mush]))\n",
    "    cm_lr = confusion_matrix(mushroom_Y[test_mush], y_predclf)\n",
    "    cm_totallr += cm_lr\n",
    "print(\"\\n\")\n",
    "print(\"Avg. Accuracy for Logistic Regression: \",np.mean(acc_res))\n",
    "print(\"Confusion Matrix for Logistic Regression: \\n\",cm_totallr)\n",
    "print(\"\\n Bernoulli Naive Bayes Accuracy\")\n",
    "\n",
    "#Naive Bayes Classifier\n",
    "cm_totalbnb= np.zeros((2,2))\n",
    "for i, (train_mush, test_mush) in enumerate(cv):\n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(mushroom_X[train_mush],mushroom_Y[train_mush])\n",
    "    y_predbnb = bnb.predict(mushroom_X[test_mush])\n",
    "    print(i+1 ,\"Trial Accuracy: \",accuracy_score(y_predbnb, mushroom_Y[test_mush]))\n",
    "    acc_res.append(accuracy_score(y_predbnb, mushroom_Y[test_mush]))\n",
    "    cm_bnb = confusion_matrix(mushroom_Y[test_mush], y_predbnb)\n",
    "    cm_totalbnb += cm_bnb\n",
    "print(\"\\n\")\n",
    "print(\"Avg. Accuracy for Bernoulli Naive Bayes: \",np.mean(acc_res))\n",
    "print(\"Confusion Matrix for Bernoulli Naive Bayes: \\n\",cm_totalbnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:**\n",
    "\n",
    "Do the example assignment in Chapter 6 of the NLTK book: http://www.nltk.org/book/ch06.html\n",
    "\n",
    "a.\tDevelop a Naïve Bayes classifier that can determine the likely gender of a given name, including at least four features using a training set consisting of 500 randomly-selected samples. List the top 10 features for this classifier along with their corresponding odds ratios. Also indicate the accuracy of this classifier on the test set:\n",
    "\n",
    "b.\tDetermine the accuracy of this classifier for all of the students in this course.\n",
    "\n",
    "c.\tFit a logistic regression model to the same dataset using the same features. Which one performs better? Why?\n",
    "\n",
    "d.\tVary the size of the training set from 100 to 2000 in increments of 100. For each increment, fit a Naïve Bayes classifier and a Logistic Regression classifier. Plot or tabulate the accuracy of each classifier. What can you conclude about the relative performance of NB vs. MaxEnt classifiers as the size of the training set increases?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Part A\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vowel_counter(word):\n",
    "    count = 0\n",
    "    vowels = set(\"aeiouAEIOU\")\n",
    "    for letter in word:\n",
    "        if letter in vowels:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'consonants_count': 2,\n",
       " 'first_two_letter': 'Za',\n",
       " 'last_letter': 'a',\n",
       " 'name_length': 4,\n",
       " 'vowel_count': 2}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gender_features(word):\n",
    "    vc = 0\n",
    "    return {'last_letter': word[-1],\n",
    "           'first_two_letter' : word[:2],\n",
    "           'name_length': len(word),\n",
    "           'vowel_count': vowel_counter(word),\n",
    "           'consonants_count': len(word)-vowel_counter(word)}\n",
    "\n",
    "gender_features('Zara')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])\n",
    "import random\n",
    "random.shuffle(labeled_names)\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "print()\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(gender_features('Neo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(gender_features('Trinity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Accuracy: 0.742\n",
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     35.5 : 1.0\n",
      "             last_letter = 'k'              male : female =     32.7 : 1.0\n",
      "        first_two_letter = 'Hu'             male : female =     17.5 : 1.0\n",
      "             last_letter = 'f'              male : female =     16.2 : 1.0\n",
      "        first_two_letter = 'Ya'             male : female =     11.9 : 1.0\n",
      "             last_letter = 'v'              male : female =     11.3 : 1.0\n",
      "             last_letter = 'p'              male : female =     11.3 : 1.0\n",
      "        first_two_letter = 'Tu'             male : female =     10.7 : 1.0\n",
      "             last_letter = 'd'              male : female =      9.8 : 1.0\n",
      "             last_letter = 'm'              male : female =      9.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier Accuracy:\",nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'last_letter': 'a', 'first_two_letter': 'Pa', 'name_length': 6, 'vowel_count': 3, 'consonants_count': 3}, 'female'), ({'last_letter': 'e', 'first_two_letter': 'Vi', 'name_length': 6, 'vowel_count': 3, 'consonants_count': 3}, 'male'), ...]\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import apply_features\n",
    "train_set = apply_features(gender_features, labeled_names[500:])\n",
    "test_set = apply_features(gender_features, labeled_names[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amal is female\n",
      "Amirah is female\n",
      "Philip is male\n",
      "Willie is female\n",
      "Ethan is female\n",
      "Devin is female\n",
      "Jane is female\n",
      "Seamus is male\n",
      "Niveditha is female\n",
      "Hyuk is male\n",
      "Hanting is male\n",
      "Xi is male\n",
      "Yiwei is female\n",
      "Zhenglin is male\n",
      "Mykola is female\n",
      "Classifier Accuracy: 0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "#Students of the course Part B\n",
    "\n",
    "course_ml = [\"Amal\",\n",
    "           \"Amirah\",\n",
    "           \"Philip\",\n",
    "           \"Willie\",\n",
    "           \"Ethan\",\n",
    "           \"Devin\",\n",
    "           \"Jane\",\n",
    "           \"Seamus\",\n",
    "           \"Niv\",\n",
    "           \"Hyuk\",\n",
    "           \"Hanting\",\n",
    "           \"Xi\",\n",
    "           \"Yiwei\",\n",
    "            \"Zhenglin\",\n",
    "           \"Mykola\"]\n",
    "\n",
    "course_ml_gender = [\"female\",\n",
    "                  \"female\",\n",
    "                  \"male\",\n",
    "                  \"male\",\n",
    "                  \"male\",\n",
    "                  \"male\",\n",
    "                  \"female\",\n",
    "                  \"male\",\n",
    "                  \"female\",\n",
    "                  \"male\",\n",
    "                  \"male\",\n",
    "                  \"male\",\n",
    "                  \"male\",\n",
    "                  \"male\",\n",
    "                  \"female\"]\n",
    "\n",
    "acc=[]\n",
    "for j, name in enumerate(course_ml):\n",
    "    print(course_ml[j] + \" is \" + classifier.classify(gender_features(course_ml[j])))\n",
    "    acc.append(classifier.classify(gender_features(course_ml[j])) == course_ml_gender[j])\n",
    "print(\"Classifier Accuracy:\",sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.368\n",
      "             2          -0.50541        0.701\n",
      "             3          -0.44519        0.775\n",
      "             4          -0.40894        0.790\n",
      "             5          -0.38564        0.796\n",
      "             6          -0.36970        0.797\n",
      "             7          -0.35823        0.801\n",
      "             8          -0.34966        0.802\n",
      "             9          -0.34305        0.804\n",
      "            10          -0.33783        0.804\n",
      "            11          -0.33362        0.805\n",
      "            12          -0.33017        0.807\n",
      "            13          -0.32730        0.808\n",
      "            14          -0.32489        0.807\n",
      "            15          -0.32284        0.807\n",
      "            16          -0.32109        0.808\n",
      "            17          -0.31957        0.808\n",
      "            18          -0.31825        0.808\n",
      "            19          -0.31709        0.808\n",
      "            20          -0.31607        0.808\n",
      "            21          -0.31517        0.809\n",
      "            22          -0.31436        0.809\n",
      "            23          -0.31364        0.808\n",
      "            24          -0.31300        0.809\n",
      "            25          -0.31241        0.809\n",
      "            26          -0.31188        0.809\n",
      "            27          -0.31140        0.809\n",
      "            28          -0.31097        0.809\n",
      "            29          -0.31057        0.809\n",
      "            30          -0.31020        0.809\n",
      "            31          -0.30986        0.810\n",
      "            32          -0.30955        0.810\n",
      "            33          -0.30926        0.810\n",
      "            34          -0.30899        0.810\n",
      "            35          -0.30874        0.810\n",
      "            36          -0.30851        0.810\n",
      "            37          -0.30829        0.810\n",
      "            38          -0.30809        0.810\n",
      "            39          -0.30790        0.810\n",
      "            40          -0.30772        0.809\n",
      "            41          -0.30755        0.809\n",
      "            42          -0.30740        0.809\n",
      "            43          -0.30725        0.809\n",
      "            44          -0.30710        0.809\n",
      "            45          -0.30697        0.809\n",
      "            46          -0.30684        0.809\n",
      "            47          -0.30672        0.809\n",
      "            48          -0.30660        0.809\n",
      "            49          -0.30649        0.809\n",
      "            50          -0.30639        0.809\n",
      "            51          -0.30629        0.809\n",
      "            52          -0.30619        0.809\n",
      "            53          -0.30610        0.809\n",
      "            54          -0.30601        0.810\n",
      "            55          -0.30593        0.810\n",
      "            56          -0.30585        0.810\n",
      "            57          -0.30577        0.810\n",
      "            58          -0.30569        0.810\n",
      "            59          -0.30562        0.810\n",
      "            60          -0.30555        0.810\n",
      "            61          -0.30548        0.810\n",
      "            62          -0.30541        0.811\n",
      "            63          -0.30535        0.811\n",
      "            64          -0.30529        0.811\n",
      "            65          -0.30523        0.811\n",
      "            66          -0.30517        0.811\n",
      "            67          -0.30512        0.811\n",
      "            68          -0.30506        0.811\n",
      "            69          -0.30501        0.811\n",
      "            70          -0.30496        0.811\n",
      "            71          -0.30491        0.811\n",
      "            72          -0.30486        0.811\n",
      "            73          -0.30481        0.811\n",
      "            74          -0.30477        0.811\n",
      "            75          -0.30472        0.811\n",
      "            76          -0.30468        0.811\n",
      "            77          -0.30464        0.811\n",
      "            78          -0.30460        0.811\n",
      "            79          -0.30456        0.811\n",
      "            80          -0.30452        0.811\n",
      "            81          -0.30448        0.811\n",
      "            82          -0.30444        0.811\n",
      "            83          -0.30441        0.811\n",
      "            84          -0.30437        0.811\n",
      "            85          -0.30434        0.811\n",
      "            86          -0.30430        0.811\n",
      "            87          -0.30427        0.811\n",
      "            88          -0.30424        0.811\n",
      "            89          -0.30421        0.811\n",
      "            90          -0.30417        0.811\n",
      "            91          -0.30414        0.811\n",
      "            92          -0.30411        0.811\n",
      "            93          -0.30409        0.811\n",
      "            94          -0.30406        0.811\n",
      "            95          -0.30403        0.811\n",
      "            96          -0.30400        0.811\n",
      "            97          -0.30397        0.811\n",
      "            98          -0.30395        0.811\n",
      "            99          -0.30392        0.811\n",
      "         Final          -0.30390        0.811\n"
     ]
    }
   ],
   "source": [
    "#Part C\n",
    "Maxent = nltk.MaxentClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.350\n",
      "             2          -0.48105        0.770\n",
      "             3          -0.40308        0.870\n",
      "             4          -0.35298        0.920\n",
      "             5          -0.31684        0.940\n",
      "             6          -0.28893        0.950\n",
      "             7          -0.26642        0.950\n",
      "             8          -0.24772        0.950\n",
      "             9          -0.23185        0.980\n",
      "            10          -0.21816        0.980\n",
      "            11          -0.20620        0.990\n",
      "            12          -0.19564        0.990\n",
      "            13          -0.18623        0.990\n",
      "            14          -0.17779        0.990\n",
      "            15          -0.17016        0.990\n",
      "            16          -0.16324        0.990\n",
      "            17          -0.15692        0.990\n",
      "            18          -0.15113        0.990\n",
      "            19          -0.14580        0.990\n",
      "            20          -0.14088        0.990\n",
      "            21          -0.13631        0.990\n",
      "            22          -0.13207        0.990\n",
      "            23          -0.12812        0.990\n",
      "            24          -0.12442        0.990\n",
      "            25          -0.12096        0.990\n",
      "            26          -0.11771        0.990\n",
      "            27          -0.11465        0.990\n",
      "            28          -0.11176        0.990\n",
      "            29          -0.10904        0.990\n",
      "            30          -0.10646        0.990\n",
      "            31          -0.10401        0.990\n",
      "            32          -0.10169        0.990\n",
      "            33          -0.09949        0.990\n",
      "            34          -0.09739        0.990\n",
      "            35          -0.09539        0.990\n",
      "            36          -0.09348        0.990\n",
      "            37          -0.09166        0.990\n",
      "            38          -0.08992        0.990\n",
      "            39          -0.08825        0.990\n",
      "            40          -0.08665        0.990\n",
      "            41          -0.08511        0.990\n",
      "            42          -0.08364        0.990\n",
      "            43          -0.08222        0.990\n",
      "            44          -0.08086        0.990\n",
      "            45          -0.07955        0.990\n",
      "            46          -0.07828        0.990\n",
      "            47          -0.07707        0.990\n",
      "            48          -0.07589        0.990\n",
      "            49          -0.07475        0.990\n",
      "            50          -0.07366        0.990\n",
      "            51          -0.07260        0.990\n",
      "            52          -0.07157        0.990\n",
      "            53          -0.07058        0.990\n",
      "            54          -0.06962        0.990\n",
      "            55          -0.06869        0.990\n",
      "            56          -0.06778        0.990\n",
      "            57          -0.06691        0.990\n",
      "            58          -0.06606        0.990\n",
      "            59          -0.06523        0.990\n",
      "            60          -0.06443        0.990\n",
      "            61          -0.06365        0.990\n",
      "            62          -0.06289        1.000\n",
      "            63          -0.06215        1.000\n",
      "            64          -0.06143        1.000\n",
      "            65          -0.06073        1.000\n",
      "            66          -0.06005        1.000\n",
      "            67          -0.05939        1.000\n",
      "            68          -0.05874        1.000\n",
      "            69          -0.05811        1.000\n",
      "            70          -0.05749        1.000\n",
      "            71          -0.05689        1.000\n",
      "            72          -0.05631        1.000\n",
      "            73          -0.05573        1.000\n",
      "            74          -0.05518        1.000\n",
      "            75          -0.05463        1.000\n",
      "            76          -0.05410        1.000\n",
      "            77          -0.05357        1.000\n",
      "            78          -0.05306        1.000\n",
      "            79          -0.05256        1.000\n",
      "            80          -0.05207        1.000\n",
      "            81          -0.05160        1.000\n",
      "            82          -0.05113        1.000\n",
      "            83          -0.05067        1.000\n",
      "            84          -0.05022        1.000\n",
      "            85          -0.04978        1.000\n",
      "            86          -0.04935        1.000\n",
      "            87          -0.04893        1.000\n",
      "            88          -0.04851        1.000\n",
      "            89          -0.04811        1.000\n",
      "            90          -0.04771        1.000\n",
      "            91          -0.04732        1.000\n",
      "            92          -0.04693        1.000\n",
      "            93          -0.04656        1.000\n",
      "            94          -0.04619        1.000\n",
      "            95          -0.04582        1.000\n",
      "            96          -0.04547        1.000\n",
      "            97          -0.04512        1.000\n",
      "            98          -0.04477        1.000\n",
      "            99          -0.04444        1.000\n",
      "         Final          -0.04410        1.000\n",
      "When training size is:  100\n",
      "Logistic Regression Accuracy = 0.713539010708822\n",
      "Naive Bayes Classifier Accuracy = 0.684089750127486\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.330\n",
      "             2          -0.43025        0.755\n",
      "             3          -0.36019        0.845\n",
      "             4          -0.31715        0.885\n",
      "             5          -0.28764        0.895\n",
      "             6          -0.26571        0.900\n",
      "             7          -0.24849        0.900\n",
      "             8          -0.23441        0.915\n",
      "             9          -0.22255        0.920\n",
      "            10          -0.21236        0.930\n",
      "            11          -0.20344        0.935\n",
      "            12          -0.19553        0.940\n",
      "            13          -0.18844        0.950\n",
      "            14          -0.18203        0.955\n",
      "            15          -0.17620        0.955\n",
      "            16          -0.17085        0.960\n",
      "            17          -0.16593        0.970\n",
      "            18          -0.16137        0.975\n",
      "            19          -0.15714        0.980\n",
      "            20          -0.15319        0.980\n",
      "            21          -0.14950        0.985\n",
      "            22          -0.14603        0.985\n",
      "            23          -0.14277        0.985\n",
      "            24          -0.13970        0.985\n",
      "            25          -0.13680        0.985\n",
      "            26          -0.13405        0.985\n",
      "            27          -0.13144        0.985\n",
      "            28          -0.12896        0.985\n",
      "            29          -0.12660        0.985\n",
      "            30          -0.12435        0.985\n",
      "            31          -0.12221        0.985\n",
      "            32          -0.12016        0.985\n",
      "            33          -0.11819        0.985\n",
      "            34          -0.11631        0.985\n",
      "            35          -0.11451        0.985\n",
      "            36          -0.11277        0.985\n",
      "            37          -0.11111        0.985\n",
      "            38          -0.10950        0.985\n",
      "            39          -0.10796        0.985\n",
      "            40          -0.10647        0.985\n",
      "            41          -0.10504        0.985\n",
      "            42          -0.10365        0.985\n",
      "            43          -0.10231        0.985\n",
      "            44          -0.10102        0.985\n",
      "            45          -0.09977        0.985\n",
      "            46          -0.09856        0.985\n",
      "            47          -0.09738        0.985\n",
      "            48          -0.09624        0.985\n",
      "            49          -0.09514        0.995\n",
      "            50          -0.09407        0.995\n",
      "            51          -0.09303        0.995\n",
      "            52          -0.09202        0.995\n",
      "            53          -0.09103        0.995\n",
      "            54          -0.09008        0.995\n",
      "            55          -0.08915        0.995\n",
      "            56          -0.08824        0.995\n",
      "            57          -0.08736        0.995\n",
      "            58          -0.08650        0.995\n",
      "            59          -0.08567        0.995\n",
      "            60          -0.08485        0.995\n",
      "            61          -0.08405        0.995\n",
      "            62          -0.08328        0.995\n",
      "            63          -0.08252        0.995\n",
      "            64          -0.08178        0.995\n",
      "            65          -0.08105        0.995\n",
      "            66          -0.08035        0.995\n",
      "            67          -0.07966        0.995\n",
      "            68          -0.07898        0.995\n",
      "            69          -0.07832        0.995\n",
      "            70          -0.07767        0.995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            71          -0.07704        0.995\n",
      "            72          -0.07642        0.995\n",
      "            73          -0.07581        0.995\n",
      "            74          -0.07522        0.995\n",
      "            75          -0.07463        0.995\n",
      "            76          -0.07406        0.995\n",
      "            77          -0.07350        0.995\n",
      "            78          -0.07295        0.995\n",
      "            79          -0.07242        0.995\n",
      "            80          -0.07189        0.995\n",
      "            81          -0.07137        0.995\n",
      "            82          -0.07086        0.995\n",
      "            83          -0.07036        0.995\n",
      "            84          -0.06987        0.995\n",
      "            85          -0.06939        0.995\n",
      "            86          -0.06891        0.995\n",
      "            87          -0.06845        0.995\n",
      "            88          -0.06799        0.995\n",
      "            89          -0.06754        0.995\n",
      "            90          -0.06710        0.995\n",
      "            91          -0.06667        0.995\n",
      "            92          -0.06624        0.995\n",
      "            93          -0.06582        0.995\n",
      "            94          -0.06541        0.995\n",
      "            95          -0.06500        0.995\n",
      "            96          -0.06460        0.995\n",
      "            97          -0.06420        0.995\n",
      "            98          -0.06382        0.995\n",
      "            99          -0.06343        0.995\n",
      "         Final          -0.06306        0.995\n",
      "When training size is:  200\n",
      "Logistic Regression Accuracy = 0.7254648760330579\n",
      "Naive Bayes Classifier Accuracy = 0.7137138429752066\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.350\n",
      "             2          -0.45909        0.720\n",
      "             3          -0.39090        0.810\n",
      "             4          -0.34981        0.850\n",
      "             5          -0.32212        0.867\n",
      "             6          -0.30181        0.873\n",
      "             7          -0.28599        0.880\n",
      "             8          -0.27315        0.893\n",
      "             9          -0.26239        0.903\n",
      "            10          -0.25318        0.907\n",
      "            11          -0.24515        0.907\n",
      "            12          -0.23805        0.907\n",
      "            13          -0.23171        0.910\n",
      "            14          -0.22599        0.917\n",
      "            15          -0.22080        0.920\n",
      "            16          -0.21604        0.920\n",
      "            17          -0.21167        0.923\n",
      "            18          -0.20762        0.923\n",
      "            19          -0.20387        0.927\n",
      "            20          -0.20037        0.930\n",
      "            21          -0.19709        0.937\n",
      "            22          -0.19402        0.937\n",
      "            23          -0.19113        0.943\n",
      "            24          -0.18840        0.943\n",
      "            25          -0.18582        0.943\n",
      "            26          -0.18338        0.943\n",
      "            27          -0.18105        0.943\n",
      "            28          -0.17884        0.943\n",
      "            29          -0.17674        0.943\n",
      "            30          -0.17473        0.943\n",
      "            31          -0.17281        0.943\n",
      "            32          -0.17097        0.943\n",
      "            33          -0.16921        0.943\n",
      "            34          -0.16751        0.943\n",
      "            35          -0.16589        0.943\n",
      "            36          -0.16432        0.943\n",
      "            37          -0.16282        0.943\n",
      "            38          -0.16137        0.943\n",
      "            39          -0.15997        0.943\n",
      "            40          -0.15862        0.943\n",
      "            41          -0.15731        0.943\n",
      "            42          -0.15605        0.943\n",
      "            43          -0.15483        0.940\n",
      "            44          -0.15365        0.940\n",
      "            45          -0.15250        0.940\n",
      "            46          -0.15139        0.940\n",
      "            47          -0.15031        0.940\n",
      "            48          -0.14927        0.940\n",
      "            49          -0.14825        0.940\n",
      "            50          -0.14726        0.943\n",
      "            51          -0.14630        0.943\n",
      "            52          -0.14537        0.943\n",
      "            53          -0.14446        0.943\n",
      "            54          -0.14357        0.943\n",
      "            55          -0.14271        0.943\n",
      "            56          -0.14187        0.943\n",
      "            57          -0.14105        0.943\n",
      "            58          -0.14025        0.943\n",
      "            59          -0.13947        0.943\n",
      "            60          -0.13871        0.947\n",
      "            61          -0.13796        0.947\n",
      "            62          -0.13724        0.947\n",
      "            63          -0.13653        0.947\n",
      "            64          -0.13583        0.947\n",
      "            65          -0.13515        0.947\n",
      "            66          -0.13449        0.947\n",
      "            67          -0.13384        0.947\n",
      "            68          -0.13320        0.947\n",
      "            69          -0.13258        0.947\n",
      "            70          -0.13197        0.947\n",
      "            71          -0.13138        0.947\n",
      "            72          -0.13079        0.947\n",
      "            73          -0.13022        0.947\n",
      "            74          -0.12966        0.947\n",
      "            75          -0.12911        0.947\n",
      "            76          -0.12857        0.950\n",
      "            77          -0.12804        0.950\n",
      "            78          -0.12752        0.950\n",
      "            79          -0.12701        0.950\n",
      "            80          -0.12651        0.950\n",
      "            81          -0.12601        0.950\n",
      "            82          -0.12553        0.950\n",
      "            83          -0.12506        0.950\n",
      "            84          -0.12459        0.950\n",
      "            85          -0.12413        0.950\n",
      "            86          -0.12368        0.950\n",
      "            87          -0.12324        0.950\n",
      "            88          -0.12281        0.950\n",
      "            89          -0.12238        0.950\n",
      "            90          -0.12196        0.950\n",
      "            91          -0.12155        0.950\n",
      "            92          -0.12114        0.950\n",
      "            93          -0.12074        0.950\n",
      "            94          -0.12035        0.950\n",
      "            95          -0.11996        0.950\n",
      "            96          -0.11958        0.950\n",
      "            97          -0.11920        0.950\n",
      "            98          -0.11883        0.950\n",
      "            99          -0.11847        0.950\n",
      "         Final          -0.11811        0.950\n",
      "When training size is:  300\n",
      "Logistic Regression Accuracy = 0.7267137624280482\n",
      "Naive Bayes Classifier Accuracy = 0.7149398220826793\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.357\n",
      "             2          -0.47128        0.733\n",
      "             3          -0.40351        0.815\n",
      "             4          -0.36267        0.843\n",
      "             5          -0.33529        0.858\n",
      "             6          -0.31540        0.863\n",
      "             7          -0.30007        0.865\n",
      "             8          -0.28776        0.875\n",
      "             9          -0.27755        0.882\n",
      "            10          -0.26888        0.887\n",
      "            11          -0.26138        0.887\n",
      "            12          -0.25481        0.887\n",
      "            13          -0.24897        0.892\n",
      "            14          -0.24373        0.895\n",
      "            15          -0.23899        0.900\n",
      "            16          -0.23468        0.900\n",
      "            17          -0.23073        0.900\n",
      "            18          -0.22709        0.902\n",
      "            19          -0.22372        0.907\n",
      "            20          -0.22059        0.905\n",
      "            21          -0.21767        0.902\n",
      "            22          -0.21493        0.902\n",
      "            23          -0.21237        0.902\n",
      "            24          -0.20995        0.905\n",
      "            25          -0.20767        0.905\n",
      "            26          -0.20551        0.905\n",
      "            27          -0.20346        0.905\n",
      "            28          -0.20151        0.905\n",
      "            29          -0.19966        0.905\n",
      "            30          -0.19790        0.907\n",
      "            31          -0.19621        0.912\n",
      "            32          -0.19460        0.912\n",
      "            33          -0.19305        0.912\n",
      "            34          -0.19157        0.912\n",
      "            35          -0.19015        0.912\n",
      "            36          -0.18878        0.912\n",
      "            37          -0.18746        0.912\n",
      "            38          -0.18620        0.912\n",
      "            39          -0.18497        0.912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            40          -0.18380        0.915\n",
      "            41          -0.18266        0.915\n",
      "            42          -0.18156        0.915\n",
      "            43          -0.18049        0.915\n",
      "            44          -0.17946        0.915\n",
      "            45          -0.17847        0.915\n",
      "            46          -0.17750        0.915\n",
      "            47          -0.17656        0.915\n",
      "            48          -0.17565        0.915\n",
      "            49          -0.17477        0.915\n",
      "            50          -0.17391        0.915\n",
      "            51          -0.17307        0.915\n",
      "            52          -0.17226        0.915\n",
      "            53          -0.17147        0.915\n",
      "            54          -0.17070        0.915\n",
      "            55          -0.16996        0.915\n",
      "            56          -0.16923        0.915\n",
      "            57          -0.16852        0.915\n",
      "            58          -0.16782        0.915\n",
      "            59          -0.16715        0.915\n",
      "            60          -0.16649        0.915\n",
      "            61          -0.16585        0.915\n",
      "            62          -0.16522        0.915\n",
      "            63          -0.16460        0.915\n",
      "            64          -0.16401        0.915\n",
      "            65          -0.16342        0.915\n",
      "            66          -0.16285        0.915\n",
      "            67          -0.16229        0.915\n",
      "            68          -0.16174        0.915\n",
      "            69          -0.16120        0.915\n",
      "            70          -0.16068        0.917\n",
      "            71          -0.16016        0.917\n",
      "            72          -0.15966        0.917\n",
      "            73          -0.15917        0.917\n",
      "            74          -0.15868        0.917\n",
      "            75          -0.15821        0.917\n",
      "            76          -0.15775        0.917\n",
      "            77          -0.15729        0.917\n",
      "            78          -0.15685        0.917\n",
      "            79          -0.15641        0.917\n",
      "            80          -0.15598        0.917\n",
      "            81          -0.15556        0.917\n",
      "            82          -0.15514        0.917\n",
      "            83          -0.15474        0.917\n",
      "            84          -0.15434        0.920\n",
      "            85          -0.15395        0.920\n",
      "            86          -0.15356        0.920\n",
      "            87          -0.15319        0.920\n",
      "            88          -0.15282        0.920\n",
      "            89          -0.15245        0.922\n",
      "            90          -0.15209        0.922\n",
      "            91          -0.15174        0.922\n",
      "            92          -0.15139        0.922\n",
      "            93          -0.15105        0.922\n",
      "            94          -0.15072        0.917\n",
      "            95          -0.15039        0.917\n",
      "            96          -0.15006        0.917\n",
      "            97          -0.14974        0.917\n",
      "            98          -0.14943        0.917\n",
      "            99          -0.14912        0.917\n",
      "         Final          -0.14882        0.917\n",
      "When training size is:  400\n",
      "Logistic Regression Accuracy = 0.7249469777306469\n",
      "Naive Bayes Classifier Accuracy = 0.7119565217391305\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.340\n",
      "             2          -0.46646        0.708\n",
      "             3          -0.40379        0.808\n",
      "             4          -0.36545        0.832\n",
      "             5          -0.33974        0.836\n",
      "             6          -0.32115        0.840\n",
      "             7          -0.30695        0.850\n",
      "             8          -0.29565        0.850\n",
      "             9          -0.28638        0.854\n",
      "            10          -0.27858        0.858\n",
      "            11          -0.27190        0.864\n",
      "            12          -0.26609        0.868\n",
      "            13          -0.26098        0.876\n",
      "            14          -0.25643        0.876\n",
      "            15          -0.25234        0.874\n",
      "            16          -0.24865        0.874\n",
      "            17          -0.24529        0.874\n",
      "            18          -0.24221        0.876\n",
      "            19          -0.23938        0.878\n",
      "            20          -0.23677        0.880\n",
      "            21          -0.23434        0.882\n",
      "            22          -0.23208        0.884\n",
      "            23          -0.22997        0.884\n",
      "            24          -0.22800        0.886\n",
      "            25          -0.22614        0.886\n",
      "            26          -0.22439        0.886\n",
      "            27          -0.22274        0.886\n",
      "            28          -0.22117        0.888\n",
      "            29          -0.21969        0.888\n",
      "            30          -0.21829        0.886\n",
      "            31          -0.21695        0.886\n",
      "            32          -0.21567        0.888\n",
      "            33          -0.21446        0.888\n",
      "            34          -0.21330        0.888\n",
      "            35          -0.21218        0.888\n",
      "            36          -0.21112        0.886\n",
      "            37          -0.21010        0.888\n",
      "            38          -0.20911        0.886\n",
      "            39          -0.20817        0.886\n",
      "            40          -0.20726        0.886\n",
      "            41          -0.20639        0.886\n",
      "            42          -0.20555        0.886\n",
      "            43          -0.20473        0.886\n",
      "            44          -0.20395        0.886\n",
      "            45          -0.20319        0.886\n",
      "            46          -0.20246        0.886\n",
      "            47          -0.20175        0.888\n",
      "            48          -0.20106        0.888\n",
      "            49          -0.20040        0.888\n",
      "            50          -0.19975        0.888\n",
      "            51          -0.19913        0.888\n",
      "            52          -0.19852        0.888\n",
      "            53          -0.19793        0.888\n",
      "            54          -0.19736        0.888\n",
      "            55          -0.19680        0.888\n",
      "            56          -0.19626        0.888\n",
      "            57          -0.19573        0.888\n",
      "            58          -0.19522        0.888\n",
      "            59          -0.19472        0.888\n",
      "            60          -0.19423        0.888\n",
      "            61          -0.19375        0.888\n",
      "            62          -0.19329        0.888\n",
      "            63          -0.19284        0.888\n",
      "            64          -0.19240        0.888\n",
      "            65          -0.19197        0.888\n",
      "            66          -0.19155        0.888\n",
      "            67          -0.19114        0.886\n",
      "            68          -0.19074        0.886\n",
      "            69          -0.19034        0.886\n",
      "            70          -0.18996        0.886\n",
      "            71          -0.18959        0.886\n",
      "            72          -0.18922        0.886\n",
      "            73          -0.18886        0.886\n",
      "            74          -0.18851        0.886\n",
      "            75          -0.18816        0.886\n",
      "            76          -0.18783        0.886\n",
      "            77          -0.18750        0.886\n",
      "            78          -0.18717        0.884\n",
      "            79          -0.18686        0.884\n",
      "            80          -0.18655        0.886\n",
      "            81          -0.18624        0.886\n",
      "            82          -0.18594        0.888\n",
      "            83          -0.18565        0.890\n",
      "            84          -0.18536        0.892\n",
      "            85          -0.18508        0.890\n",
      "            86          -0.18480        0.890\n",
      "            87          -0.18453        0.890\n",
      "            88          -0.18426        0.892\n",
      "            89          -0.18400        0.892\n",
      "            90          -0.18374        0.892\n",
      "            91          -0.18349        0.892\n",
      "            92          -0.18324        0.892\n",
      "            93          -0.18299        0.892\n",
      "            94          -0.18275        0.892\n",
      "            95          -0.18252        0.892\n",
      "            96          -0.18228        0.892\n",
      "            97          -0.18206        0.892\n",
      "            98          -0.18183        0.892\n",
      "            99          -0.18161        0.892\n",
      "         Final          -0.18139        0.892\n",
      "When training size is:  500\n",
      "Logistic Regression Accuracy = 0.7337452982267598\n",
      "Naive Bayes Classifier Accuracy = 0.7243417517463729\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.345\n",
      "             2          -0.47548        0.710\n",
      "             3          -0.41159        0.812\n",
      "             4          -0.37221        0.828\n",
      "             5          -0.34588        0.835\n",
      "             6          -0.32700        0.837\n",
      "             7          -0.31271        0.845\n",
      "             8          -0.30145        0.847\n",
      "             9          -0.29230        0.845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            10          -0.28466        0.853\n",
      "            11          -0.27817        0.862\n",
      "            12          -0.27256        0.862\n",
      "            13          -0.26765        0.862\n",
      "            14          -0.26329        0.867\n",
      "            15          -0.25940        0.870\n",
      "            16          -0.25590        0.873\n",
      "            17          -0.25272        0.873\n",
      "            18          -0.24981        0.873\n",
      "            19          -0.24715        0.875\n",
      "            20          -0.24469        0.878\n",
      "            21          -0.24241        0.877\n",
      "            22          -0.24029        0.877\n",
      "            23          -0.23831        0.877\n",
      "            24          -0.23646        0.877\n",
      "            25          -0.23473        0.878\n",
      "            26          -0.23309        0.877\n",
      "            27          -0.23155        0.877\n",
      "            28          -0.23010        0.878\n",
      "            29          -0.22872        0.880\n",
      "            30          -0.22741        0.880\n",
      "            31          -0.22617        0.880\n",
      "            32          -0.22498        0.882\n",
      "            33          -0.22385        0.882\n",
      "            34          -0.22278        0.882\n",
      "            35          -0.22175        0.882\n",
      "            36          -0.22076        0.882\n",
      "            37          -0.21982        0.882\n",
      "            38          -0.21891        0.883\n",
      "            39          -0.21804        0.883\n",
      "            40          -0.21720        0.887\n",
      "            41          -0.21640        0.887\n",
      "            42          -0.21562        0.887\n",
      "            43          -0.21487        0.887\n",
      "            44          -0.21415        0.887\n",
      "            45          -0.21345        0.887\n",
      "            46          -0.21278        0.888\n",
      "            47          -0.21213        0.888\n",
      "            48          -0.21150        0.888\n",
      "            49          -0.21089        0.888\n",
      "            50          -0.21030        0.888\n",
      "            51          -0.20973        0.888\n",
      "            52          -0.20917        0.888\n",
      "            53          -0.20863        0.888\n",
      "            54          -0.20811        0.888\n",
      "            55          -0.20760        0.888\n",
      "            56          -0.20711        0.888\n",
      "            57          -0.20663        0.888\n",
      "            58          -0.20616        0.888\n",
      "            59          -0.20571        0.888\n",
      "            60          -0.20526        0.888\n",
      "            61          -0.20483        0.888\n",
      "            62          -0.20441        0.888\n",
      "            63          -0.20400        0.888\n",
      "            64          -0.20360        0.888\n",
      "            65          -0.20321        0.890\n",
      "            66          -0.20283        0.892\n",
      "            67          -0.20246        0.892\n",
      "            68          -0.20210        0.893\n",
      "            69          -0.20174        0.893\n",
      "            70          -0.20140        0.893\n",
      "            71          -0.20106        0.893\n",
      "            72          -0.20073        0.893\n",
      "            73          -0.20040        0.893\n",
      "            74          -0.20009        0.893\n",
      "            75          -0.19978        0.893\n",
      "            76          -0.19948        0.893\n",
      "            77          -0.19918        0.893\n",
      "            78          -0.19889        0.893\n",
      "            79          -0.19860        0.893\n",
      "            80          -0.19833        0.893\n",
      "            81          -0.19805        0.895\n",
      "            82          -0.19778        0.895\n",
      "            83          -0.19752        0.895\n",
      "            84          -0.19726        0.895\n",
      "            85          -0.19701        0.895\n",
      "            86          -0.19676        0.895\n",
      "            87          -0.19652        0.895\n",
      "            88          -0.19628        0.895\n",
      "            89          -0.19605        0.895\n",
      "            90          -0.19582        0.895\n",
      "            91          -0.19559        0.895\n",
      "            92          -0.19537        0.895\n",
      "            93          -0.19515        0.895\n",
      "            94          -0.19494        0.895\n",
      "            95          -0.19473        0.895\n",
      "            96          -0.19452        0.895\n",
      "            97          -0.19432        0.895\n",
      "            98          -0.19412        0.895\n",
      "            99          -0.19392        0.895\n",
      "         Final          -0.19373        0.895\n",
      "When training size is:  600\n",
      "Logistic Regression Accuracy = 0.7396514161220044\n",
      "Naive Bayes Classifier Accuracy = 0.732979302832244\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.353\n",
      "             2          -0.48146        0.710\n",
      "             3          -0.41627        0.819\n",
      "             4          -0.37631        0.834\n",
      "             5          -0.34961        0.839\n",
      "             6          -0.33044        0.844\n",
      "             7          -0.31588        0.843\n",
      "             8          -0.30437        0.844\n",
      "             9          -0.29498        0.850\n",
      "            10          -0.28712        0.853\n",
      "            11          -0.28042        0.859\n",
      "            12          -0.27463        0.861\n",
      "            13          -0.26954        0.864\n",
      "            14          -0.26503        0.863\n",
      "            15          -0.26100        0.864\n",
      "            16          -0.25737        0.866\n",
      "            17          -0.25407        0.866\n",
      "            18          -0.25106        0.869\n",
      "            19          -0.24830        0.870\n",
      "            20          -0.24575        0.870\n",
      "            21          -0.24340        0.871\n",
      "            22          -0.24121        0.873\n",
      "            23          -0.23917        0.873\n",
      "            24          -0.23727        0.873\n",
      "            25          -0.23548        0.874\n",
      "            26          -0.23380        0.874\n",
      "            27          -0.23223        0.874\n",
      "            28          -0.23073        0.874\n",
      "            29          -0.22932        0.876\n",
      "            30          -0.22799        0.876\n",
      "            31          -0.22672        0.876\n",
      "            32          -0.22551        0.876\n",
      "            33          -0.22437        0.876\n",
      "            34          -0.22327        0.876\n",
      "            35          -0.22223        0.876\n",
      "            36          -0.22123        0.876\n",
      "            37          -0.22027        0.876\n",
      "            38          -0.21936        0.876\n",
      "            39          -0.21848        0.876\n",
      "            40          -0.21763        0.877\n",
      "            41          -0.21682        0.877\n",
      "            42          -0.21604        0.877\n",
      "            43          -0.21529        0.877\n",
      "            44          -0.21457        0.877\n",
      "            45          -0.21387        0.877\n",
      "            46          -0.21320        0.877\n",
      "            47          -0.21255        0.877\n",
      "            48          -0.21192        0.879\n",
      "            49          -0.21131        0.879\n",
      "            50          -0.21073        0.880\n",
      "            51          -0.21016        0.880\n",
      "            52          -0.20960        0.880\n",
      "            53          -0.20907        0.880\n",
      "            54          -0.20855        0.880\n",
      "            55          -0.20805        0.880\n",
      "            56          -0.20756        0.880\n",
      "            57          -0.20708        0.877\n",
      "            58          -0.20662        0.877\n",
      "            59          -0.20617        0.879\n",
      "            60          -0.20574        0.879\n",
      "            61          -0.20531        0.877\n",
      "            62          -0.20490        0.877\n",
      "            63          -0.20450        0.877\n",
      "            64          -0.20411        0.877\n",
      "            65          -0.20372        0.877\n",
      "            66          -0.20335        0.877\n",
      "            67          -0.20299        0.877\n",
      "            68          -0.20263        0.877\n",
      "            69          -0.20229        0.877\n",
      "            70          -0.20195        0.877\n",
      "            71          -0.20162        0.877\n",
      "            72          -0.20129        0.877\n",
      "            73          -0.20098        0.877\n",
      "            74          -0.20067        0.877\n",
      "            75          -0.20037        0.877\n",
      "            76          -0.20008        0.877\n",
      "            77          -0.19979        0.877\n",
      "            78          -0.19951        0.879\n",
      "            79          -0.19923        0.879\n",
      "            80          -0.19896        0.879\n",
      "            81          -0.19869        0.879\n",
      "            82          -0.19844        0.879\n",
      "            83          -0.19818        0.879\n",
      "            84          -0.19793        0.879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            85          -0.19769        0.879\n",
      "            86          -0.19745        0.879\n",
      "            87          -0.19722        0.879\n",
      "            88          -0.19699        0.879\n",
      "            89          -0.19676        0.879\n",
      "            90          -0.19654        0.879\n",
      "            91          -0.19632        0.879\n",
      "            92          -0.19611        0.879\n",
      "            93          -0.19590        0.879\n",
      "            94          -0.19569        0.879\n",
      "            95          -0.19549        0.880\n",
      "            96          -0.19529        0.880\n",
      "            97          -0.19510        0.880\n",
      "            98          -0.19491        0.880\n",
      "            99          -0.19472        0.880\n",
      "         Final          -0.19454        0.880\n",
      "When training size is:  700\n",
      "Logistic Regression Accuracy = 0.7444781888459415\n",
      "Naive Bayes Classifier Accuracy = 0.7332965212589729\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.361\n",
      "             2          -0.49821        0.701\n",
      "             3          -0.43444        0.802\n",
      "             4          -0.39477        0.819\n",
      "             5          -0.36811        0.824\n",
      "             6          -0.34897        0.824\n",
      "             7          -0.33450        0.829\n",
      "             8          -0.32310        0.836\n",
      "             9          -0.31384        0.838\n",
      "            10          -0.30613        0.839\n",
      "            11          -0.29959        0.840\n",
      "            12          -0.29394        0.844\n",
      "            13          -0.28900        0.846\n",
      "            14          -0.28464        0.849\n",
      "            15          -0.28074        0.851\n",
      "            16          -0.27723        0.851\n",
      "            17          -0.27405        0.855\n",
      "            18          -0.27116        0.854\n",
      "            19          -0.26850        0.855\n",
      "            20          -0.26606        0.856\n",
      "            21          -0.26380        0.859\n",
      "            22          -0.26170        0.859\n",
      "            23          -0.25975        0.861\n",
      "            24          -0.25792        0.860\n",
      "            25          -0.25622        0.860\n",
      "            26          -0.25461        0.860\n",
      "            27          -0.25310        0.860\n",
      "            28          -0.25167        0.863\n",
      "            29          -0.25033        0.863\n",
      "            30          -0.24905        0.861\n",
      "            31          -0.24784        0.860\n",
      "            32          -0.24669        0.860\n",
      "            33          -0.24560        0.859\n",
      "            34          -0.24455        0.859\n",
      "            35          -0.24356        0.859\n",
      "            36          -0.24261        0.859\n",
      "            37          -0.24170        0.860\n",
      "            38          -0.24083        0.858\n",
      "            39          -0.23999        0.858\n",
      "            40          -0.23919        0.858\n",
      "            41          -0.23842        0.859\n",
      "            42          -0.23768        0.859\n",
      "            43          -0.23697        0.859\n",
      "            44          -0.23629        0.860\n",
      "            45          -0.23563        0.861\n",
      "            46          -0.23499        0.861\n",
      "            47          -0.23438        0.861\n",
      "            48          -0.23378        0.861\n",
      "            49          -0.23321        0.861\n",
      "            50          -0.23266        0.861\n",
      "            51          -0.23212        0.860\n",
      "            52          -0.23160        0.860\n",
      "            53          -0.23110        0.860\n",
      "            54          -0.23061        0.860\n",
      "            55          -0.23014        0.860\n",
      "            56          -0.22968        0.860\n",
      "            57          -0.22924        0.859\n",
      "            58          -0.22880        0.859\n",
      "            59          -0.22838        0.859\n",
      "            60          -0.22798        0.860\n",
      "            61          -0.22758        0.860\n",
      "            62          -0.22719        0.860\n",
      "            63          -0.22682        0.860\n",
      "            64          -0.22645        0.860\n",
      "            65          -0.22610        0.859\n",
      "            66          -0.22575        0.859\n",
      "            67          -0.22541        0.859\n",
      "            68          -0.22509        0.859\n",
      "            69          -0.22476        0.859\n",
      "            70          -0.22445        0.859\n",
      "            71          -0.22415        0.859\n",
      "            72          -0.22385        0.859\n",
      "            73          -0.22356        0.859\n",
      "            74          -0.22327        0.859\n",
      "            75          -0.22300        0.859\n",
      "            76          -0.22273        0.859\n",
      "            77          -0.22246        0.859\n",
      "            78          -0.22220        0.859\n",
      "            79          -0.22195        0.859\n",
      "            80          -0.22170        0.859\n",
      "            81          -0.22146        0.859\n",
      "            82          -0.22122        0.859\n",
      "            83          -0.22099        0.859\n",
      "            84          -0.22076        0.859\n",
      "            85          -0.22054        0.859\n",
      "            86          -0.22032        0.859\n",
      "            87          -0.22011        0.859\n",
      "            88          -0.21990        0.859\n",
      "            89          -0.21970        0.859\n",
      "            90          -0.21950        0.859\n",
      "            91          -0.21930        0.859\n",
      "            92          -0.21911        0.859\n",
      "            93          -0.21892        0.859\n",
      "            94          -0.21873        0.859\n",
      "            95          -0.21855        0.859\n",
      "            96          -0.21837        0.859\n",
      "            97          -0.21820        0.859\n",
      "            98          -0.21802        0.859\n",
      "            99          -0.21786        0.859\n",
      "         Final          -0.21769        0.859\n",
      "When training size is:  800\n",
      "Logistic Regression Accuracy = 0.7455207166853304\n",
      "Naive Bayes Classifier Accuracy = 0.7347424412094065\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.360\n",
      "             2          -0.48905        0.710\n",
      "             3          -0.42281        0.813\n",
      "             4          -0.38229        0.826\n",
      "             5          -0.35557        0.827\n",
      "             6          -0.33669        0.834\n",
      "             7          -0.32261        0.842\n",
      "             8          -0.31167        0.842\n",
      "             9          -0.30287        0.846\n",
      "            10          -0.29561        0.844\n",
      "            11          -0.28949        0.846\n",
      "            12          -0.28424        0.846\n",
      "            13          -0.27968        0.849\n",
      "            14          -0.27566        0.849\n",
      "            15          -0.27209        0.852\n",
      "            16          -0.26889        0.853\n",
      "            17          -0.26600        0.852\n",
      "            18          -0.26337        0.854\n",
      "            19          -0.26097        0.854\n",
      "            20          -0.25876        0.857\n",
      "            21          -0.25672        0.858\n",
      "            22          -0.25483        0.858\n",
      "            23          -0.25308        0.857\n",
      "            24          -0.25144        0.857\n",
      "            25          -0.24991        0.857\n",
      "            26          -0.24847        0.857\n",
      "            27          -0.24712        0.857\n",
      "            28          -0.24585        0.856\n",
      "            29          -0.24464        0.856\n",
      "            30          -0.24350        0.858\n",
      "            31          -0.24242        0.859\n",
      "            32          -0.24140        0.858\n",
      "            33          -0.24042        0.857\n",
      "            34          -0.23950        0.857\n",
      "            35          -0.23861        0.857\n",
      "            36          -0.23776        0.859\n",
      "            37          -0.23695        0.859\n",
      "            38          -0.23618        0.859\n",
      "            39          -0.23544        0.859\n",
      "            40          -0.23473        0.859\n",
      "            41          -0.23404        0.859\n",
      "            42          -0.23338        0.860\n",
      "            43          -0.23275        0.861\n",
      "            44          -0.23214        0.861\n",
      "            45          -0.23156        0.861\n",
      "            46          -0.23099        0.861\n",
      "            47          -0.23045        0.861\n",
      "            48          -0.22992        0.861\n",
      "            49          -0.22941        0.861\n",
      "            50          -0.22892        0.861\n",
      "            51          -0.22844        0.860\n",
      "            52          -0.22798        0.860\n",
      "            53          -0.22753        0.860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            54          -0.22710        0.860\n",
      "            55          -0.22668        0.860\n",
      "            56          -0.22627        0.859\n",
      "            57          -0.22588        0.859\n",
      "            58          -0.22550        0.859\n",
      "            59          -0.22512        0.859\n",
      "            60          -0.22476        0.859\n",
      "            61          -0.22441        0.859\n",
      "            62          -0.22407        0.859\n",
      "            63          -0.22373        0.859\n",
      "            64          -0.22341        0.859\n",
      "            65          -0.22309        0.859\n",
      "            66          -0.22279        0.859\n",
      "            67          -0.22249        0.859\n",
      "            68          -0.22219        0.860\n",
      "            69          -0.22191        0.860\n",
      "            70          -0.22163        0.860\n",
      "            71          -0.22136        0.860\n",
      "            72          -0.22110        0.859\n",
      "            73          -0.22084        0.859\n",
      "            74          -0.22059        0.859\n",
      "            75          -0.22034        0.859\n",
      "            76          -0.22010        0.859\n",
      "            77          -0.21986        0.859\n",
      "            78          -0.21963        0.859\n",
      "            79          -0.21941        0.859\n",
      "            80          -0.21919        0.859\n",
      "            81          -0.21898        0.857\n",
      "            82          -0.21877        0.857\n",
      "            83          -0.21856        0.857\n",
      "            84          -0.21836        0.857\n",
      "            85          -0.21816        0.857\n",
      "            86          -0.21797        0.857\n",
      "            87          -0.21778        0.857\n",
      "            88          -0.21759        0.857\n",
      "            89          -0.21741        0.857\n",
      "            90          -0.21723        0.857\n",
      "            91          -0.21706        0.857\n",
      "            92          -0.21689        0.857\n",
      "            93          -0.21672        0.858\n",
      "            94          -0.21655        0.858\n",
      "            95          -0.21639        0.858\n",
      "            96          -0.21623        0.858\n",
      "            97          -0.21608        0.858\n",
      "            98          -0.21593        0.858\n",
      "            99          -0.21578        0.859\n",
      "         Final          -0.21563        0.859\n",
      "When training size is:  900\n",
      "Logistic Regression Accuracy = 0.7536910846110164\n",
      "Naive Bayes Classifier Accuracy = 0.7423339011925043\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.361\n",
      "             2          -0.49156        0.707\n",
      "             3          -0.42630        0.810\n",
      "             4          -0.38654        0.817\n",
      "             5          -0.36039        0.825\n",
      "             6          -0.34195        0.829\n",
      "             7          -0.32823        0.837\n",
      "             8          -0.31759        0.841\n",
      "             9          -0.30905        0.842\n",
      "            10          -0.30202        0.843\n",
      "            11          -0.29611        0.841\n",
      "            12          -0.29106        0.846\n",
      "            13          -0.28667        0.847\n",
      "            14          -0.28283        0.848\n",
      "            15          -0.27942        0.848\n",
      "            16          -0.27637        0.851\n",
      "            17          -0.27363        0.852\n",
      "            18          -0.27115        0.852\n",
      "            19          -0.26888        0.850\n",
      "            20          -0.26681        0.851\n",
      "            21          -0.26491        0.850\n",
      "            22          -0.26315        0.849\n",
      "            23          -0.26151        0.850\n",
      "            24          -0.26000        0.850\n",
      "            25          -0.25859        0.851\n",
      "            26          -0.25726        0.850\n",
      "            27          -0.25603        0.851\n",
      "            28          -0.25486        0.851\n",
      "            29          -0.25377        0.851\n",
      "            30          -0.25274        0.852\n",
      "            31          -0.25176        0.853\n",
      "            32          -0.25084        0.853\n",
      "            33          -0.24996        0.854\n",
      "            34          -0.24913        0.853\n",
      "            35          -0.24834        0.852\n",
      "            36          -0.24759        0.852\n",
      "            37          -0.24687        0.852\n",
      "            38          -0.24619        0.852\n",
      "            39          -0.24553        0.853\n",
      "            40          -0.24491        0.852\n",
      "            41          -0.24431        0.851\n",
      "            42          -0.24373        0.851\n",
      "            43          -0.24318        0.850\n",
      "            44          -0.24265        0.850\n",
      "            45          -0.24214        0.850\n",
      "            46          -0.24165        0.850\n",
      "            47          -0.24118        0.850\n",
      "            48          -0.24073        0.850\n",
      "            49          -0.24029        0.850\n",
      "            50          -0.23987        0.849\n",
      "            51          -0.23946        0.849\n",
      "            52          -0.23907        0.849\n",
      "            53          -0.23869        0.850\n",
      "            54          -0.23832        0.850\n",
      "            55          -0.23796        0.850\n",
      "            56          -0.23762        0.850\n",
      "            57          -0.23728        0.851\n",
      "            58          -0.23696        0.851\n",
      "            59          -0.23665        0.851\n",
      "            60          -0.23634        0.851\n",
      "            61          -0.23605        0.851\n",
      "            62          -0.23576        0.851\n",
      "            63          -0.23548        0.851\n",
      "            64          -0.23521        0.851\n",
      "            65          -0.23495        0.851\n",
      "            66          -0.23469        0.851\n",
      "            67          -0.23444        0.851\n",
      "            68          -0.23420        0.850\n",
      "            69          -0.23397        0.849\n",
      "            70          -0.23374        0.849\n",
      "            71          -0.23351        0.849\n",
      "            72          -0.23329        0.849\n",
      "            73          -0.23308        0.850\n",
      "            74          -0.23287        0.850\n",
      "            75          -0.23267        0.850\n",
      "            76          -0.23247        0.850\n",
      "            77          -0.23228        0.850\n",
      "            78          -0.23209        0.850\n",
      "            79          -0.23191        0.851\n",
      "            80          -0.23173        0.851\n",
      "            81          -0.23155        0.851\n",
      "            82          -0.23138        0.852\n",
      "            83          -0.23121        0.852\n",
      "            84          -0.23105        0.852\n",
      "            85          -0.23089        0.852\n",
      "            86          -0.23073        0.852\n",
      "            87          -0.23058        0.852\n",
      "            88          -0.23043        0.852\n",
      "            89          -0.23028        0.852\n",
      "            90          -0.23014        0.852\n",
      "            91          -0.23000        0.852\n",
      "            92          -0.22986        0.852\n",
      "            93          -0.22972        0.852\n",
      "            94          -0.22959        0.852\n",
      "            95          -0.22946        0.853\n",
      "            96          -0.22933        0.853\n",
      "            97          -0.22921        0.853\n",
      "            98          -0.22908        0.853\n",
      "            99          -0.22896        0.853\n",
      "         Final          -0.22884        0.853\n",
      "When training size is:  1000\n",
      "Logistic Regression Accuracy = 0.7508640552995391\n",
      "Naive Bayes Classifier Accuracy = 0.7446716589861752\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.359\n",
      "             2          -0.49100        0.712\n",
      "             3          -0.42628        0.809\n",
      "             4          -0.38669        0.819\n",
      "             5          -0.36055        0.827\n",
      "             6          -0.34208        0.836\n",
      "             7          -0.32832        0.837\n",
      "             8          -0.31764        0.839\n",
      "             9          -0.30907        0.842\n",
      "            10          -0.30201        0.845\n",
      "            11          -0.29609        0.847\n",
      "            12          -0.29103        0.848\n",
      "            13          -0.28664        0.851\n",
      "            14          -0.28280        0.848\n",
      "            15          -0.27940        0.851\n",
      "            16          -0.27636        0.852\n",
      "            17          -0.27363        0.852\n",
      "            18          -0.27116        0.851\n",
      "            19          -0.26891        0.852\n",
      "            20          -0.26685        0.852\n",
      "            21          -0.26496        0.851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            22          -0.26321        0.852\n",
      "            23          -0.26160        0.851\n",
      "            24          -0.26010        0.851\n",
      "            25          -0.25870        0.849\n",
      "            26          -0.25739        0.850\n",
      "            27          -0.25617        0.850\n",
      "            28          -0.25502        0.850\n",
      "            29          -0.25393        0.850\n",
      "            30          -0.25291        0.851\n",
      "            31          -0.25195        0.850\n",
      "            32          -0.25104        0.850\n",
      "            33          -0.25018        0.850\n",
      "            34          -0.24936        0.851\n",
      "            35          -0.24858        0.851\n",
      "            36          -0.24783        0.852\n",
      "            37          -0.24713        0.851\n",
      "            38          -0.24645        0.852\n",
      "            39          -0.24581        0.852\n",
      "            40          -0.24519        0.851\n",
      "            41          -0.24460        0.851\n",
      "            42          -0.24403        0.850\n",
      "            43          -0.24349        0.851\n",
      "            44          -0.24297        0.851\n",
      "            45          -0.24247        0.851\n",
      "            46          -0.24199        0.851\n",
      "            47          -0.24152        0.852\n",
      "            48          -0.24108        0.852\n",
      "            49          -0.24065        0.852\n",
      "            50          -0.24023        0.852\n",
      "            51          -0.23983        0.852\n",
      "            52          -0.23945        0.852\n",
      "            53          -0.23907        0.852\n",
      "            54          -0.23871        0.851\n",
      "            55          -0.23836        0.851\n",
      "            56          -0.23802        0.851\n",
      "            57          -0.23770        0.851\n",
      "            58          -0.23738        0.851\n",
      "            59          -0.23707        0.850\n",
      "            60          -0.23677        0.850\n",
      "            61          -0.23648        0.849\n",
      "            62          -0.23620        0.849\n",
      "            63          -0.23593        0.849\n",
      "            64          -0.23566        0.849\n",
      "            65          -0.23541        0.849\n",
      "            66          -0.23516        0.849\n",
      "            67          -0.23491        0.849\n",
      "            68          -0.23468        0.849\n",
      "            69          -0.23445        0.849\n",
      "            70          -0.23422        0.849\n",
      "            71          -0.23400        0.849\n",
      "            72          -0.23379        0.849\n",
      "            73          -0.23358        0.849\n",
      "            74          -0.23338        0.849\n",
      "            75          -0.23318        0.849\n",
      "            76          -0.23299        0.849\n",
      "            77          -0.23280        0.849\n",
      "            78          -0.23262        0.849\n",
      "            79          -0.23244        0.848\n",
      "            80          -0.23227        0.848\n",
      "            81          -0.23209        0.848\n",
      "            82          -0.23193        0.848\n",
      "            83          -0.23176        0.848\n",
      "            84          -0.23161        0.848\n",
      "            85          -0.23145        0.848\n",
      "            86          -0.23130        0.848\n",
      "            87          -0.23115        0.848\n",
      "            88          -0.23100        0.848\n",
      "            89          -0.23086        0.848\n",
      "            90          -0.23072        0.849\n",
      "            91          -0.23058        0.849\n",
      "            92          -0.23045        0.849\n",
      "            93          -0.23032        0.849\n",
      "            94          -0.23019        0.849\n",
      "            95          -0.23006        0.849\n",
      "            96          -0.22994        0.849\n",
      "            97          -0.22982        0.849\n",
      "            98          -0.22970        0.849\n",
      "            99          -0.22958        0.849\n",
      "         Final          -0.22947        0.849\n",
      "When training size is:  1100\n",
      "Logistic Regression Accuracy = 0.7527761542957335\n",
      "Naive Bayes Classifier Accuracy = 0.7462010520163647\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.363\n",
      "             2          -0.48959        0.713\n",
      "             3          -0.42384        0.807\n",
      "             4          -0.38419        0.817\n",
      "             5          -0.35827        0.827\n",
      "             6          -0.34007        0.833\n",
      "             7          -0.32656        0.839\n",
      "             8          -0.31610        0.837\n",
      "             9          -0.30772        0.841\n",
      "            10          -0.30083        0.843\n",
      "            11          -0.29505        0.841\n",
      "            12          -0.29011        0.847\n",
      "            13          -0.28583        0.847\n",
      "            14          -0.28208        0.848\n",
      "            15          -0.27876        0.848\n",
      "            16          -0.27580        0.848\n",
      "            17          -0.27314        0.850\n",
      "            18          -0.27073        0.848\n",
      "            19          -0.26853        0.849\n",
      "            20          -0.26653        0.847\n",
      "            21          -0.26468        0.847\n",
      "            22          -0.26298        0.848\n",
      "            23          -0.26141        0.848\n",
      "            24          -0.25995        0.847\n",
      "            25          -0.25859        0.848\n",
      "            26          -0.25732        0.849\n",
      "            27          -0.25613        0.850\n",
      "            28          -0.25501        0.851\n",
      "            29          -0.25396        0.851\n",
      "            30          -0.25297        0.850\n",
      "            31          -0.25204        0.850\n",
      "            32          -0.25115        0.850\n",
      "            33          -0.25032        0.852\n",
      "            34          -0.24952        0.852\n",
      "            35          -0.24877        0.852\n",
      "            36          -0.24805        0.852\n",
      "            37          -0.24737        0.852\n",
      "            38          -0.24672        0.852\n",
      "            39          -0.24610        0.853\n",
      "            40          -0.24551        0.853\n",
      "            41          -0.24494        0.853\n",
      "            42          -0.24439        0.852\n",
      "            43          -0.24387        0.853\n",
      "            44          -0.24337        0.853\n",
      "            45          -0.24289        0.853\n",
      "            46          -0.24243        0.853\n",
      "            47          -0.24199        0.853\n",
      "            48          -0.24156        0.853\n",
      "            49          -0.24115        0.853\n",
      "            50          -0.24076        0.853\n",
      "            51          -0.24037        0.853\n",
      "            52          -0.24001        0.853\n",
      "            53          -0.23965        0.853\n",
      "            54          -0.23931        0.853\n",
      "            55          -0.23897        0.853\n",
      "            56          -0.23865        0.853\n",
      "            57          -0.23834        0.853\n",
      "            58          -0.23804        0.853\n",
      "            59          -0.23775        0.853\n",
      "            60          -0.23747        0.853\n",
      "            61          -0.23719        0.852\n",
      "            62          -0.23693        0.852\n",
      "            63          -0.23667        0.853\n",
      "            64          -0.23642        0.853\n",
      "            65          -0.23618        0.852\n",
      "            66          -0.23594        0.852\n",
      "            67          -0.23571        0.852\n",
      "            68          -0.23549        0.852\n",
      "            69          -0.23527        0.852\n",
      "            70          -0.23506        0.852\n",
      "            71          -0.23485        0.852\n",
      "            72          -0.23465        0.852\n",
      "            73          -0.23446        0.852\n",
      "            74          -0.23427        0.852\n",
      "            75          -0.23408        0.852\n",
      "            76          -0.23390        0.852\n",
      "            77          -0.23373        0.852\n",
      "            78          -0.23355        0.852\n",
      "            79          -0.23339        0.852\n",
      "            80          -0.23322        0.852\n",
      "            81          -0.23306        0.852\n",
      "            82          -0.23291        0.852\n",
      "            83          -0.23275        0.852\n",
      "            84          -0.23261        0.852\n",
      "            85          -0.23246        0.852\n",
      "            86          -0.23232        0.852\n",
      "            87          -0.23218        0.852\n",
      "            88          -0.23204        0.852\n",
      "            89          -0.23191        0.851\n",
      "            90          -0.23178        0.851\n",
      "            91          -0.23165        0.851\n",
      "            92          -0.23153        0.851\n",
      "            93          -0.23140        0.851\n",
      "            94          -0.23129        0.851\n",
      "            95          -0.23117        0.851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            96          -0.23105        0.851\n",
      "            97          -0.23094        0.851\n",
      "            98          -0.23083        0.851\n",
      "            99          -0.23072        0.851\n",
      "         Final          -0.23062        0.851\n",
      "When training size is:  1200\n",
      "Logistic Regression Accuracy = 0.7513345195729537\n",
      "Naive Bayes Classifier Accuracy = 0.7498517200474496\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.361\n",
      "             2          -0.48812        0.709\n",
      "             3          -0.42378        0.800\n",
      "             4          -0.38491        0.815\n",
      "             5          -0.35950        0.822\n",
      "             6          -0.34167        0.828\n",
      "             7          -0.32845        0.832\n",
      "             8          -0.31824        0.832\n",
      "             9          -0.31007        0.835\n",
      "            10          -0.30338        0.835\n",
      "            11          -0.29777        0.838\n",
      "            12          -0.29299        0.836\n",
      "            13          -0.28886        0.837\n",
      "            14          -0.28525        0.838\n",
      "            15          -0.28206        0.842\n",
      "            16          -0.27923        0.843\n",
      "            17          -0.27668        0.845\n",
      "            18          -0.27438        0.845\n",
      "            19          -0.27228        0.845\n",
      "            20          -0.27038        0.844\n",
      "            21          -0.26862        0.844\n",
      "            22          -0.26701        0.847\n",
      "            23          -0.26552        0.848\n",
      "            24          -0.26413        0.849\n",
      "            25          -0.26285        0.850\n",
      "            26          -0.26165        0.851\n",
      "            27          -0.26052        0.851\n",
      "            28          -0.25947        0.851\n",
      "            29          -0.25848        0.851\n",
      "            30          -0.25754        0.851\n",
      "            31          -0.25666        0.850\n",
      "            32          -0.25583        0.851\n",
      "            33          -0.25504        0.852\n",
      "            34          -0.25429        0.852\n",
      "            35          -0.25358        0.852\n",
      "            36          -0.25290        0.852\n",
      "            37          -0.25226        0.852\n",
      "            38          -0.25165        0.850\n",
      "            39          -0.25106        0.850\n",
      "            40          -0.25050        0.851\n",
      "            41          -0.24996        0.851\n",
      "            42          -0.24945        0.852\n",
      "            43          -0.24896        0.852\n",
      "            44          -0.24848        0.852\n",
      "            45          -0.24803        0.851\n",
      "            46          -0.24759        0.852\n",
      "            47          -0.24717        0.852\n",
      "            48          -0.24677        0.852\n",
      "            49          -0.24638        0.852\n",
      "            50          -0.24600        0.852\n",
      "            51          -0.24564        0.852\n",
      "            52          -0.24529        0.852\n",
      "            53          -0.24495        0.852\n",
      "            54          -0.24462        0.852\n",
      "            55          -0.24431        0.852\n",
      "            56          -0.24400        0.852\n",
      "            57          -0.24371        0.852\n",
      "            58          -0.24342        0.852\n",
      "            59          -0.24314        0.852\n",
      "            60          -0.24287        0.852\n",
      "            61          -0.24261        0.852\n",
      "            62          -0.24235        0.852\n",
      "            63          -0.24211        0.852\n",
      "            64          -0.24187        0.852\n",
      "            65          -0.24164        0.852\n",
      "            66          -0.24141        0.852\n",
      "            67          -0.24119        0.852\n",
      "            68          -0.24097        0.853\n",
      "            69          -0.24077        0.853\n",
      "            70          -0.24056        0.853\n",
      "            71          -0.24036        0.853\n",
      "            72          -0.24017        0.852\n",
      "            73          -0.23998        0.852\n",
      "            74          -0.23980        0.852\n",
      "            75          -0.23962        0.852\n",
      "            76          -0.23945        0.852\n",
      "            77          -0.23928        0.852\n",
      "            78          -0.23911        0.852\n",
      "            79          -0.23895        0.853\n",
      "            80          -0.23879        0.853\n",
      "            81          -0.23864        0.852\n",
      "            82          -0.23848        0.852\n",
      "            83          -0.23834        0.852\n",
      "            84          -0.23819        0.852\n",
      "            85          -0.23805        0.852\n",
      "            86          -0.23791        0.852\n",
      "            87          -0.23778        0.852\n",
      "            88          -0.23764        0.852\n",
      "            89          -0.23751        0.852\n",
      "            90          -0.23739        0.851\n",
      "            91          -0.23726        0.851\n",
      "            92          -0.23714        0.851\n",
      "            93          -0.23702        0.851\n",
      "            94          -0.23691        0.851\n",
      "            95          -0.23679        0.851\n",
      "            96          -0.23668        0.851\n",
      "            97          -0.23657        0.851\n",
      "            98          -0.23646        0.851\n",
      "            99          -0.23635        0.850\n",
      "         Final          -0.23625        0.850\n",
      "When training size is:  1300\n",
      "Logistic Regression Accuracy = 0.7551173991571343\n",
      "Naive Bayes Classifier Accuracy = 0.7519566526189043\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.373\n",
      "             2          -0.49965        0.714\n",
      "             3          -0.43345        0.800\n",
      "             4          -0.39343        0.812\n",
      "             5          -0.36727        0.823\n",
      "             6          -0.34893        0.829\n",
      "             7          -0.33537        0.832\n",
      "             8          -0.32491        0.834\n",
      "             9          -0.31657        0.838\n",
      "            10          -0.30974        0.839\n",
      "            11          -0.30403        0.840\n",
      "            12          -0.29917        0.841\n",
      "            13          -0.29498        0.844\n",
      "            14          -0.29132        0.846\n",
      "            15          -0.28809        0.846\n",
      "            16          -0.28521        0.846\n",
      "            17          -0.28263        0.846\n",
      "            18          -0.28030        0.848\n",
      "            19          -0.27819        0.849\n",
      "            20          -0.27626        0.849\n",
      "            21          -0.27449        0.850\n",
      "            22          -0.27286        0.849\n",
      "            23          -0.27136        0.849\n",
      "            24          -0.26996        0.849\n",
      "            25          -0.26866        0.849\n",
      "            26          -0.26745        0.849\n",
      "            27          -0.26632        0.849\n",
      "            28          -0.26526        0.849\n",
      "            29          -0.26426        0.849\n",
      "            30          -0.26332        0.849\n",
      "            31          -0.26243        0.849\n",
      "            32          -0.26160        0.849\n",
      "            33          -0.26080        0.848\n",
      "            34          -0.26005        0.848\n",
      "            35          -0.25934        0.849\n",
      "            36          -0.25866        0.848\n",
      "            37          -0.25801        0.848\n",
      "            38          -0.25739        0.848\n",
      "            39          -0.25680        0.849\n",
      "            40          -0.25624        0.849\n",
      "            41          -0.25570        0.849\n",
      "            42          -0.25519        0.849\n",
      "            43          -0.25469        0.849\n",
      "            44          -0.25422        0.847\n",
      "            45          -0.25376        0.847\n",
      "            46          -0.25332        0.847\n",
      "            47          -0.25290        0.847\n",
      "            48          -0.25249        0.846\n",
      "            49          -0.25210        0.846\n",
      "            50          -0.25172        0.846\n",
      "            51          -0.25136        0.846\n",
      "            52          -0.25101        0.846\n",
      "            53          -0.25067        0.846\n",
      "            54          -0.25034        0.846\n",
      "            55          -0.25002        0.846\n",
      "            56          -0.24971        0.846\n",
      "            57          -0.24942        0.846\n",
      "            58          -0.24913        0.846\n",
      "            59          -0.24885        0.846\n",
      "            60          -0.24858        0.846\n",
      "            61          -0.24831        0.846\n",
      "            62          -0.24806        0.846\n",
      "            63          -0.24781        0.846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            64          -0.24757        0.846\n",
      "            65          -0.24733        0.846\n",
      "            66          -0.24711        0.846\n",
      "            67          -0.24688        0.846\n",
      "            68          -0.24667        0.846\n",
      "            69          -0.24646        0.846\n",
      "            70          -0.24625        0.846\n",
      "            71          -0.24605        0.846\n",
      "            72          -0.24586        0.846\n",
      "            73          -0.24567        0.846\n",
      "            74          -0.24548        0.846\n",
      "            75          -0.24530        0.846\n",
      "            76          -0.24513        0.846\n",
      "            77          -0.24496        0.846\n",
      "            78          -0.24479        0.846\n",
      "            79          -0.24462        0.846\n",
      "            80          -0.24446        0.846\n",
      "            81          -0.24431        0.846\n",
      "            82          -0.24416        0.846\n",
      "            83          -0.24401        0.846\n",
      "            84          -0.24386        0.846\n",
      "            85          -0.24372        0.846\n",
      "            86          -0.24358        0.846\n",
      "            87          -0.24344        0.846\n",
      "            88          -0.24331        0.846\n",
      "            89          -0.24317        0.846\n",
      "            90          -0.24305        0.846\n",
      "            91          -0.24292        0.846\n",
      "            92          -0.24280        0.846\n",
      "            93          -0.24268        0.846\n",
      "            94          -0.24256        0.846\n",
      "            95          -0.24244        0.846\n",
      "            96          -0.24233        0.846\n",
      "            97          -0.24222        0.846\n",
      "            98          -0.24211        0.846\n",
      "            99          -0.24200        0.846\n",
      "         Final          -0.24189        0.846\n",
      "When training size is:  1400\n",
      "Logistic Regression Accuracy = 0.7571821515892421\n",
      "Naive Bayes Classifier Accuracy = 0.7551955990220048\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.372\n",
      "             2          -0.49898        0.713\n",
      "             3          -0.43265        0.802\n",
      "             4          -0.39251        0.810\n",
      "             5          -0.36625        0.821\n",
      "             6          -0.34785        0.826\n",
      "             7          -0.33424        0.831\n",
      "             8          -0.32374        0.833\n",
      "             9          -0.31537        0.835\n",
      "            10          -0.30851        0.837\n",
      "            11          -0.30279        0.839\n",
      "            12          -0.29791        0.842\n",
      "            13          -0.29371        0.843\n",
      "            14          -0.29004        0.843\n",
      "            15          -0.28681        0.845\n",
      "            16          -0.28393        0.847\n",
      "            17          -0.28135        0.849\n",
      "            18          -0.27902        0.850\n",
      "            19          -0.27691        0.850\n",
      "            20          -0.27498        0.850\n",
      "            21          -0.27322        0.851\n",
      "            22          -0.27160        0.852\n",
      "            23          -0.27010        0.854\n",
      "            24          -0.26871        0.853\n",
      "            25          -0.26742        0.853\n",
      "            26          -0.26622        0.854\n",
      "            27          -0.26509        0.854\n",
      "            28          -0.26404        0.854\n",
      "            29          -0.26305        0.853\n",
      "            30          -0.26213        0.853\n",
      "            31          -0.26125        0.854\n",
      "            32          -0.26042        0.855\n",
      "            33          -0.25964        0.855\n",
      "            34          -0.25890        0.855\n",
      "            35          -0.25820        0.855\n",
      "            36          -0.25753        0.855\n",
      "            37          -0.25690        0.855\n",
      "            38          -0.25629        0.855\n",
      "            39          -0.25572        0.855\n",
      "            40          -0.25517        0.855\n",
      "            41          -0.25464        0.855\n",
      "            42          -0.25414        0.856\n",
      "            43          -0.25366        0.856\n",
      "            44          -0.25320        0.856\n",
      "            45          -0.25276        0.856\n",
      "            46          -0.25233        0.855\n",
      "            47          -0.25192        0.855\n",
      "            48          -0.25153        0.855\n",
      "            49          -0.25115        0.855\n",
      "            50          -0.25079        0.854\n",
      "            51          -0.25044        0.854\n",
      "            52          -0.25010        0.853\n",
      "            53          -0.24978        0.853\n",
      "            54          -0.24946        0.853\n",
      "            55          -0.24916        0.853\n",
      "            56          -0.24887        0.853\n",
      "            57          -0.24858        0.853\n",
      "            58          -0.24831        0.853\n",
      "            59          -0.24804        0.853\n",
      "            60          -0.24779        0.852\n",
      "            61          -0.24754        0.851\n",
      "            62          -0.24729        0.851\n",
      "            63          -0.24706        0.851\n",
      "            64          -0.24683        0.851\n",
      "            65          -0.24661        0.851\n",
      "            66          -0.24640        0.851\n",
      "            67          -0.24619        0.851\n",
      "            68          -0.24599        0.851\n",
      "            69          -0.24579        0.851\n",
      "            70          -0.24560        0.851\n",
      "            71          -0.24541        0.851\n",
      "            72          -0.24523        0.851\n",
      "            73          -0.24506        0.851\n",
      "            74          -0.24488        0.851\n",
      "            75          -0.24472        0.851\n",
      "            76          -0.24455        0.851\n",
      "            77          -0.24439        0.851\n",
      "            78          -0.24424        0.850\n",
      "            79          -0.24409        0.850\n",
      "            80          -0.24394        0.850\n",
      "            81          -0.24380        0.850\n",
      "            82          -0.24366        0.850\n",
      "            83          -0.24352        0.850\n",
      "            84          -0.24338        0.850\n",
      "            85          -0.24325        0.851\n",
      "            86          -0.24312        0.851\n",
      "            87          -0.24300        0.851\n",
      "            88          -0.24288        0.851\n",
      "            89          -0.24276        0.851\n",
      "            90          -0.24264        0.851\n",
      "            91          -0.24253        0.851\n",
      "            92          -0.24241        0.851\n",
      "            93          -0.24230        0.851\n",
      "            94          -0.24220        0.851\n",
      "            95          -0.24209        0.851\n",
      "            96          -0.24199        0.851\n",
      "            97          -0.24189        0.851\n",
      "            98          -0.24179        0.851\n",
      "            99          -0.24169        0.851\n",
      "         Final          -0.24160        0.851\n",
      "When training size is:  1500\n",
      "Logistic Regression Accuracy = 0.7560521415270018\n",
      "Naive Bayes Classifier Accuracy = 0.7577591558038486\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.367\n",
      "             2          -0.49649        0.706\n",
      "             3          -0.43161        0.796\n",
      "             4          -0.39222        0.810\n",
      "             5          -0.36644        0.820\n",
      "             6          -0.34838        0.825\n",
      "             7          -0.33505        0.826\n",
      "             8          -0.32479        0.825\n",
      "             9          -0.31663        0.830\n",
      "            10          -0.30997        0.831\n",
      "            11          -0.30443        0.831\n",
      "            12          -0.29972        0.834\n",
      "            13          -0.29568        0.838\n",
      "            14          -0.29216        0.838\n",
      "            15          -0.28907        0.838\n",
      "            16          -0.28633        0.843\n",
      "            17          -0.28388        0.844\n",
      "            18          -0.28167        0.846\n",
      "            19          -0.27967        0.846\n",
      "            20          -0.27786        0.847\n",
      "            21          -0.27620        0.847\n",
      "            22          -0.27467        0.847\n",
      "            23          -0.27327        0.846\n",
      "            24          -0.27197        0.846\n",
      "            25          -0.27076        0.846\n",
      "            26          -0.26964        0.846\n",
      "            27          -0.26859        0.846\n",
      "            28          -0.26762        0.846\n",
      "            29          -0.26670        0.846\n",
      "            30          -0.26584        0.848\n",
      "            31          -0.26502        0.849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            32          -0.26426        0.849\n",
      "            33          -0.26354        0.849\n",
      "            34          -0.26285        0.849\n",
      "            35          -0.26220        0.849\n",
      "            36          -0.26159        0.848\n",
      "            37          -0.26100        0.848\n",
      "            38          -0.26045        0.848\n",
      "            39          -0.25991        0.848\n",
      "            40          -0.25941        0.848\n",
      "            41          -0.25893        0.848\n",
      "            42          -0.25846        0.848\n",
      "            43          -0.25802        0.848\n",
      "            44          -0.25760        0.848\n",
      "            45          -0.25719        0.847\n",
      "            46          -0.25680        0.846\n",
      "            47          -0.25643        0.846\n",
      "            48          -0.25607        0.846\n",
      "            49          -0.25572        0.846\n",
      "            50          -0.25539        0.846\n",
      "            51          -0.25507        0.847\n",
      "            52          -0.25476        0.848\n",
      "            53          -0.25446        0.848\n",
      "            54          -0.25417        0.848\n",
      "            55          -0.25390        0.848\n",
      "            56          -0.25363        0.847\n",
      "            57          -0.25337        0.847\n",
      "            58          -0.25312        0.848\n",
      "            59          -0.25287        0.848\n",
      "            60          -0.25264        0.848\n",
      "            61          -0.25241        0.848\n",
      "            62          -0.25219        0.848\n",
      "            63          -0.25197        0.848\n",
      "            64          -0.25177        0.847\n",
      "            65          -0.25156        0.847\n",
      "            66          -0.25137        0.847\n",
      "            67          -0.25118        0.846\n",
      "            68          -0.25099        0.846\n",
      "            69          -0.25081        0.846\n",
      "            70          -0.25064        0.846\n",
      "            71          -0.25047        0.846\n",
      "            72          -0.25030        0.846\n",
      "            73          -0.25014        0.846\n",
      "            74          -0.24998        0.845\n",
      "            75          -0.24983        0.845\n",
      "            76          -0.24968        0.845\n",
      "            77          -0.24953        0.845\n",
      "            78          -0.24939        0.845\n",
      "            79          -0.24925        0.844\n",
      "            80          -0.24912        0.844\n",
      "            81          -0.24899        0.844\n",
      "            82          -0.24886        0.844\n",
      "            83          -0.24873        0.844\n",
      "            84          -0.24861        0.844\n",
      "            85          -0.24849        0.844\n",
      "            86          -0.24837        0.844\n",
      "            87          -0.24825        0.844\n",
      "            88          -0.24814        0.844\n",
      "            89          -0.24803        0.844\n",
      "            90          -0.24792        0.844\n",
      "            91          -0.24782        0.844\n",
      "            92          -0.24771        0.844\n",
      "            93          -0.24761        0.845\n",
      "            94          -0.24751        0.845\n",
      "            95          -0.24742        0.845\n",
      "            96          -0.24732        0.845\n",
      "            97          -0.24723        0.844\n",
      "            98          -0.24714        0.844\n",
      "            99          -0.24705        0.844\n",
      "         Final          -0.24696        0.844\n",
      "When training size is:  1600\n",
      "Logistic Regression Accuracy = 0.7563051702395964\n",
      "Naive Bayes Classifier Accuracy = 0.7599306431273645\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.368\n",
      "             2          -0.49836        0.701\n",
      "             3          -0.43472        0.794\n",
      "             4          -0.39605        0.806\n",
      "             5          -0.37073        0.817\n",
      "             6          -0.35302        0.824\n",
      "             7          -0.33995        0.825\n",
      "             8          -0.32991        0.825\n",
      "             9          -0.32195        0.828\n",
      "            10          -0.31547        0.829\n",
      "            11          -0.31008        0.832\n",
      "            12          -0.30553        0.834\n",
      "            13          -0.30163        0.835\n",
      "            14          -0.29824        0.837\n",
      "            15          -0.29527        0.836\n",
      "            16          -0.29264        0.835\n",
      "            17          -0.29030        0.835\n",
      "            18          -0.28819        0.836\n",
      "            19          -0.28628        0.836\n",
      "            20          -0.28455        0.835\n",
      "            21          -0.28297        0.835\n",
      "            22          -0.28152        0.836\n",
      "            23          -0.28018        0.836\n",
      "            24          -0.27895        0.836\n",
      "            25          -0.27780        0.836\n",
      "            26          -0.27673        0.836\n",
      "            27          -0.27574        0.836\n",
      "            28          -0.27481        0.838\n",
      "            29          -0.27394        0.838\n",
      "            30          -0.27312        0.839\n",
      "            31          -0.27235        0.839\n",
      "            32          -0.27162        0.838\n",
      "            33          -0.27093        0.839\n",
      "            34          -0.27028        0.840\n",
      "            35          -0.26966        0.840\n",
      "            36          -0.26908        0.841\n",
      "            37          -0.26852        0.841\n",
      "            38          -0.26799        0.841\n",
      "            39          -0.26748        0.841\n",
      "            40          -0.26700        0.842\n",
      "            41          -0.26654        0.841\n",
      "            42          -0.26610        0.841\n",
      "            43          -0.26568        0.841\n",
      "            44          -0.26528        0.842\n",
      "            45          -0.26489        0.841\n",
      "            46          -0.26452        0.841\n",
      "            47          -0.26417        0.841\n",
      "            48          -0.26382        0.841\n",
      "            49          -0.26349        0.841\n",
      "            50          -0.26318        0.841\n",
      "            51          -0.26287        0.841\n",
      "            52          -0.26258        0.841\n",
      "            53          -0.26230        0.841\n",
      "            54          -0.26202        0.841\n",
      "            55          -0.26176        0.841\n",
      "            56          -0.26150        0.841\n",
      "            57          -0.26126        0.841\n",
      "            58          -0.26102        0.841\n",
      "            59          -0.26079        0.841\n",
      "            60          -0.26057        0.841\n",
      "            61          -0.26035        0.841\n",
      "            62          -0.26014        0.841\n",
      "            63          -0.25994        0.841\n",
      "            64          -0.25974        0.841\n",
      "            65          -0.25955        0.841\n",
      "            66          -0.25937        0.841\n",
      "            67          -0.25919        0.841\n",
      "            68          -0.25901        0.841\n",
      "            69          -0.25884        0.841\n",
      "            70          -0.25868        0.841\n",
      "            71          -0.25852        0.842\n",
      "            72          -0.25836        0.842\n",
      "            73          -0.25821        0.843\n",
      "            74          -0.25806        0.843\n",
      "            75          -0.25792        0.843\n",
      "            76          -0.25778        0.843\n",
      "            77          -0.25764        0.843\n",
      "            78          -0.25751        0.843\n",
      "            79          -0.25738        0.843\n",
      "            80          -0.25725        0.843\n",
      "            81          -0.25713        0.843\n",
      "            82          -0.25701        0.843\n",
      "            83          -0.25689        0.843\n",
      "            84          -0.25677        0.843\n",
      "            85          -0.25666        0.843\n",
      "            86          -0.25655        0.843\n",
      "            87          -0.25644        0.843\n",
      "            88          -0.25634        0.843\n",
      "            89          -0.25624        0.843\n",
      "            90          -0.25614        0.843\n",
      "            91          -0.25604        0.843\n",
      "            92          -0.25594        0.844\n",
      "            93          -0.25585        0.843\n",
      "            94          -0.25576        0.843\n",
      "            95          -0.25567        0.843\n",
      "            96          -0.25558        0.843\n",
      "            97          -0.25549        0.843\n",
      "            98          -0.25541        0.843\n",
      "            99          -0.25532        0.843\n",
      "         Final          -0.25524        0.842\n",
      "When training size is:  1700\n",
      "Logistic Regression Accuracy = 0.7626521460602178\n",
      "Naive Bayes Classifier Accuracy = 0.7634529147982063\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             2          -0.50102        0.705\n",
      "             3          -0.43809        0.794\n",
      "             4          -0.39967        0.803\n",
      "             5          -0.37443        0.809\n",
      "             6          -0.35673        0.816\n",
      "             7          -0.34366        0.819\n",
      "             8          -0.33362        0.821\n",
      "             9          -0.32565        0.823\n",
      "            10          -0.31917        0.823\n",
      "            11          -0.31379        0.827\n",
      "            12          -0.30925        0.830\n",
      "            13          -0.30536        0.831\n",
      "            14          -0.30199        0.832\n",
      "            15          -0.29903        0.833\n",
      "            16          -0.29642        0.833\n",
      "            17          -0.29409        0.833\n",
      "            18          -0.29200        0.835\n",
      "            19          -0.29012        0.836\n",
      "            20          -0.28840        0.836\n",
      "            21          -0.28684        0.835\n",
      "            22          -0.28541        0.834\n",
      "            23          -0.28409        0.834\n",
      "            24          -0.28288        0.835\n",
      "            25          -0.28175        0.835\n",
      "            26          -0.28070        0.834\n",
      "            27          -0.27972        0.834\n",
      "            28          -0.27881        0.836\n",
      "            29          -0.27796        0.837\n",
      "            30          -0.27715        0.837\n",
      "            31          -0.27640        0.837\n",
      "            32          -0.27568        0.837\n",
      "            33          -0.27501        0.837\n",
      "            34          -0.27437        0.837\n",
      "            35          -0.27377        0.838\n",
      "            36          -0.27320        0.838\n",
      "            37          -0.27266        0.838\n",
      "            38          -0.27214        0.838\n",
      "            39          -0.27165        0.838\n",
      "            40          -0.27118        0.838\n",
      "            41          -0.27073        0.838\n",
      "            42          -0.27030        0.838\n",
      "            43          -0.26989        0.839\n",
      "            44          -0.26950        0.838\n",
      "            45          -0.26912        0.838\n",
      "            46          -0.26876        0.838\n",
      "            47          -0.26841        0.839\n",
      "            48          -0.26808        0.839\n",
      "            49          -0.26776        0.839\n",
      "            50          -0.26745        0.839\n",
      "            51          -0.26715        0.839\n",
      "            52          -0.26687        0.839\n",
      "            53          -0.26659        0.838\n",
      "            54          -0.26633        0.838\n",
      "            55          -0.26607        0.839\n",
      "            56          -0.26582        0.838\n",
      "            57          -0.26558        0.839\n",
      "            58          -0.26535        0.838\n",
      "            59          -0.26513        0.838\n",
      "            60          -0.26491        0.838\n",
      "            61          -0.26470        0.838\n",
      "            62          -0.26450        0.837\n",
      "            63          -0.26430        0.837\n",
      "            64          -0.26411        0.838\n",
      "            65          -0.26392        0.838\n",
      "            66          -0.26374        0.838\n",
      "            67          -0.26357        0.838\n",
      "            68          -0.26340        0.838\n",
      "            69          -0.26323        0.838\n",
      "            70          -0.26307        0.838\n",
      "            71          -0.26292        0.838\n",
      "            72          -0.26277        0.838\n",
      "            73          -0.26262        0.838\n",
      "            74          -0.26247        0.838\n",
      "            75          -0.26233        0.838\n",
      "            76          -0.26220        0.838\n",
      "            77          -0.26207        0.838\n",
      "            78          -0.26194        0.838\n",
      "            79          -0.26181        0.838\n",
      "            80          -0.26169        0.838\n",
      "            81          -0.26157        0.838\n",
      "            82          -0.26145        0.838\n",
      "            83          -0.26133        0.838\n",
      "            84          -0.26122        0.839\n",
      "            85          -0.26111        0.839\n",
      "            86          -0.26101        0.839\n",
      "            87          -0.26090        0.839\n",
      "            88          -0.26080        0.839\n",
      "            89          -0.26070        0.839\n",
      "            90          -0.26060        0.839\n",
      "            91          -0.26051        0.839\n",
      "            92          -0.26041        0.839\n",
      "            93          -0.26032        0.839\n",
      "            94          -0.26023        0.838\n",
      "            95          -0.26014        0.838\n",
      "            96          -0.26006        0.838\n",
      "            97          -0.25997        0.838\n",
      "            98          -0.25989        0.838\n",
      "            99          -0.25981        0.838\n",
      "         Final          -0.25973        0.838\n",
      "When training size is:  1800\n",
      "Logistic Regression Accuracy = 0.7610677083333334\n",
      "Naive Bayes Classifier Accuracy = 0.7623697916666666\n",
      "------\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.371\n",
      "             2          -0.50367        0.704\n",
      "             3          -0.44083        0.793\n",
      "             4          -0.40264        0.801\n",
      "             5          -0.37767        0.806\n",
      "             6          -0.36023        0.811\n",
      "             7          -0.34741        0.817\n",
      "             8          -0.33760        0.818\n",
      "             9          -0.32986        0.822\n",
      "            10          -0.32359        0.824\n",
      "            11          -0.31840        0.828\n",
      "            12          -0.31405        0.827\n",
      "            13          -0.31033        0.828\n",
      "            14          -0.30711        0.828\n",
      "            15          -0.30431        0.830\n",
      "            16          -0.30184        0.831\n",
      "            17          -0.29964        0.831\n",
      "            18          -0.29768        0.830\n",
      "            19          -0.29591        0.830\n",
      "            20          -0.29430        0.831\n",
      "            21          -0.29284        0.830\n",
      "            22          -0.29151        0.830\n",
      "            23          -0.29028        0.830\n",
      "            24          -0.28915        0.830\n",
      "            25          -0.28810        0.831\n",
      "            26          -0.28713        0.830\n",
      "            27          -0.28622        0.831\n",
      "            28          -0.28537        0.831\n",
      "            29          -0.28458        0.831\n",
      "            30          -0.28383        0.831\n",
      "            31          -0.28313        0.831\n",
      "            32          -0.28247        0.831\n",
      "            33          -0.28184        0.832\n",
      "            34          -0.28125        0.832\n",
      "            35          -0.28069        0.832\n",
      "            36          -0.28016        0.832\n",
      "            37          -0.27965        0.833\n",
      "            38          -0.27917        0.833\n",
      "            39          -0.27871        0.833\n",
      "            40          -0.27827        0.833\n",
      "            41          -0.27785        0.833\n",
      "            42          -0.27745        0.833\n",
      "            43          -0.27707        0.834\n",
      "            44          -0.27670        0.833\n",
      "            45          -0.27635        0.833\n",
      "            46          -0.27601        0.833\n",
      "            47          -0.27568        0.833\n",
      "            48          -0.27537        0.834\n",
      "            49          -0.27507        0.834\n",
      "            50          -0.27478        0.834\n",
      "            51          -0.27450        0.834\n",
      "            52          -0.27423        0.834\n",
      "            53          -0.27397        0.834\n",
      "            54          -0.27372        0.834\n",
      "            55          -0.27347        0.835\n",
      "            56          -0.27324        0.836\n",
      "            57          -0.27301        0.836\n",
      "            58          -0.27279        0.836\n",
      "            59          -0.27258        0.836\n",
      "            60          -0.27237        0.836\n",
      "            61          -0.27217        0.836\n",
      "            62          -0.27198        0.836\n",
      "            63          -0.27179        0.836\n",
      "            64          -0.27161        0.836\n",
      "            65          -0.27143        0.836\n",
      "            66          -0.27126        0.836\n",
      "            67          -0.27110        0.836\n",
      "            68          -0.27093        0.836\n",
      "            69          -0.27077        0.836\n",
      "            70          -0.27062        0.836\n",
      "            71          -0.27047        0.836\n",
      "            72          -0.27033        0.836\n",
      "            73          -0.27018        0.836\n",
      "            74          -0.27005        0.836\n",
      "            75          -0.26991        0.836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            76          -0.26978        0.836\n",
      "            77          -0.26965        0.836\n",
      "            78          -0.26953        0.836\n",
      "            79          -0.26941        0.836\n",
      "            80          -0.26929        0.836\n",
      "            81          -0.26917        0.836\n",
      "            82          -0.26906        0.836\n",
      "            83          -0.26895        0.836\n",
      "            84          -0.26884        0.836\n",
      "            85          -0.26873        0.836\n",
      "            86          -0.26863        0.836\n",
      "            87          -0.26853        0.836\n",
      "            88          -0.26843        0.836\n",
      "            89          -0.26834        0.836\n",
      "            90          -0.26824        0.836\n",
      "            91          -0.26815        0.836\n",
      "            92          -0.26806        0.836\n",
      "            93          -0.26797        0.836\n",
      "            94          -0.26788        0.836\n",
      "            95          -0.26780        0.836\n",
      "            96          -0.26771        0.836\n",
      "            97          -0.26763        0.836\n",
      "            98          -0.26755        0.837\n",
      "            99          -0.26747        0.837\n",
      "         Final          -0.26740        0.837\n",
      "When training size is:  1900\n",
      "Logistic Regression Accuracy = 0.7627399073461284\n",
      "Naive Bayes Classifier Accuracy = 0.7672071475843812\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "#Part D\n",
    "# Setting the parameter for the for loop\n",
    "initial_test_size = 100\n",
    "final_test_size = 2000 \n",
    "interval = 100\n",
    "\n",
    "graphA_y = []\n",
    "graphB_y = []\n",
    "graph_x = []\n",
    "\n",
    "for size in range(initial_test_size, final_test_size, interval):\n",
    "    # Create new Logistic Regression instance\n",
    "    train_set = featuresets[:size]\n",
    "    test_set = featuresets[size:]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    Maxent = nltk.MaxentClassifier.train(train_set)\n",
    "    \n",
    "    graph_x.append(size)\n",
    "    \n",
    "    graphA_y.append(nltk.classify.accuracy(classifier, test_set))\n",
    "    graphB_y.append(nltk.classify.accuracy(Maxent, test_set))\n",
    "    \n",
    "    print(\"When training size is: \", size)        \n",
    "    print(\"Logistic Regression Accuracy =\",nltk.classify.accuracy(Maxent, test_set))  \n",
    "    print(\"Naive Bayes Classifier Accuracy =\", nltk.classify.accuracy(classifier, test_set))\n",
    "    print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "19\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEjCAYAAAD31uwUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2485658a748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the graph\n",
    "#print(len(graph_x))\n",
    "#print(len(graphA_y))\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Classification Accuracies')\n",
    "graph = fig.add_subplot(111)\n",
    "graph.scatter(graph_x, graphA_y, c='b', marker=\"s\", label=\"Logistic Regression\")\n",
    "graph.scatter(graph_x, graphB_y, c='r', marker=\"o\", label=\"Naive Bayes\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:**\n",
    "\n",
    "Fit several logistic regression models and (Gaussian) naïve Bayes models to the Pima diabetes dataset from the UCI machine learning repository, varying training set size from 10 to 500 in units of 10. Create a plot comparing how average accuracy from each type model varies with training set size. When does each model perform better and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When training size is:  10\n",
      "Logistic Regression Accuracy = 0.58\n",
      "Naive Bayes Accuracy = 0.64\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  20\n",
      "Logistic Regression Accuracy = 0.65\n",
      "Naive Bayes Accuracy = 0.69\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  30\n",
      "Logistic Regression Accuracy = 0.61\n",
      "Naive Bayes Accuracy = 0.68\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  40\n",
      "Logistic Regression Accuracy = 0.67\n",
      "Naive Bayes Accuracy = 0.69\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  50\n",
      "Logistic Regression Accuracy = 0.75\n",
      "Naive Bayes Accuracy = 0.68\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  60\n",
      "Logistic Regression Accuracy = 0.69\n",
      "Naive Bayes Accuracy = 0.73\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  70\n",
      "Logistic Regression Accuracy = 0.69\n",
      "Naive Bayes Accuracy = 0.73\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  80\n",
      "Logistic Regression Accuracy = 0.71\n",
      "Naive Bayes Accuracy = 0.75\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  90\n",
      "Logistic Regression Accuracy = 0.63\n",
      "Naive Bayes Accuracy = 0.69\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  100\n",
      "Logistic Regression Accuracy = 0.74\n",
      "Naive Bayes Accuracy = 0.76\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  110\n",
      "Logistic Regression Accuracy = 0.75\n",
      "Naive Bayes Accuracy = 0.75\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  120\n",
      "Logistic Regression Accuracy = 0.72\n",
      "Naive Bayes Accuracy = 0.75\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  130\n",
      "Logistic Regression Accuracy = 0.62\n",
      "Naive Bayes Accuracy = 0.71\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  140\n",
      "Logistic Regression Accuracy = 0.70\n",
      "Naive Bayes Accuracy = 0.73\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  150\n",
      "Logistic Regression Accuracy = 0.75\n",
      "Naive Bayes Accuracy = 0.75\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  160\n",
      "Logistic Regression Accuracy = 0.75\n",
      "Naive Bayes Accuracy = 0.74\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  170\n",
      "Logistic Regression Accuracy = 0.75\n",
      "Naive Bayes Accuracy = 0.76\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  180\n",
      "Logistic Regression Accuracy = 0.72\n",
      "Naive Bayes Accuracy = 0.71\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  190\n",
      "Logistic Regression Accuracy = 0.76\n",
      "Naive Bayes Accuracy = 0.76\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  200\n",
      "Logistic Regression Accuracy = 0.75\n",
      "Naive Bayes Accuracy = 0.75\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  210\n",
      "Logistic Regression Accuracy = 0.76\n",
      "Naive Bayes Accuracy = 0.74\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  220\n",
      "Logistic Regression Accuracy = 0.76\n",
      "Naive Bayes Accuracy = 0.74\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  230\n",
      "Logistic Regression Accuracy = 0.75\n",
      "Naive Bayes Accuracy = 0.70\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  240\n",
      "Logistic Regression Accuracy = 0.81\n",
      "Naive Bayes Accuracy = 0.80\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  250\n",
      "Logistic Regression Accuracy = 0.73\n",
      "Naive Bayes Accuracy = 0.71\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  260\n",
      "Logistic Regression Accuracy = 0.75\n",
      "Naive Bayes Accuracy = 0.74\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  270\n",
      "Logistic Regression Accuracy = 0.71\n",
      "Naive Bayes Accuracy = 0.73\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  280\n",
      "Logistic Regression Accuracy = 0.77\n",
      "Naive Bayes Accuracy = 0.79\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  290\n",
      "Logistic Regression Accuracy = 0.73\n",
      "Naive Bayes Accuracy = 0.71\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  300\n",
      "Logistic Regression Accuracy = 0.74\n",
      "Naive Bayes Accuracy = 0.75\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  310\n",
      "Logistic Regression Accuracy = 0.76\n",
      "Naive Bayes Accuracy = 0.76\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  320\n",
      "Logistic Regression Accuracy = 0.76\n",
      "Naive Bayes Accuracy = 0.71\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  330\n",
      "Logistic Regression Accuracy = 0.81\n",
      "Naive Bayes Accuracy = 0.78\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  340\n",
      "Logistic Regression Accuracy = 0.78\n",
      "Naive Bayes Accuracy = 0.74\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  350\n",
      "Logistic Regression Accuracy = 0.76\n",
      "Naive Bayes Accuracy = 0.78\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  360\n",
      "Logistic Regression Accuracy = 0.75\n",
      "Naive Bayes Accuracy = 0.73\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  370\n",
      "Logistic Regression Accuracy = 0.77\n",
      "Naive Bayes Accuracy = 0.70\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  380\n",
      "Logistic Regression Accuracy = 0.81\n",
      "Naive Bayes Accuracy = 0.78\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  390\n",
      "Logistic Regression Accuracy = 0.72\n",
      "Naive Bayes Accuracy = 0.73\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  400\n",
      "Logistic Regression Accuracy = 0.72\n",
      "Naive Bayes Accuracy = 0.72\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  410\n",
      "Logistic Regression Accuracy = 0.75\n",
      "Naive Bayes Accuracy = 0.74\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  420\n",
      "Logistic Regression Accuracy = 0.73\n",
      "Naive Bayes Accuracy = 0.74\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  430\n",
      "Logistic Regression Accuracy = 0.78\n",
      "Naive Bayes Accuracy = 0.76\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  440\n",
      "Logistic Regression Accuracy = 0.78\n",
      "Naive Bayes Accuracy = 0.74\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  450\n",
      "Logistic Regression Accuracy = 0.75\n",
      "Naive Bayes Accuracy = 0.74\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  460\n",
      "Logistic Regression Accuracy = 0.72\n",
      "Naive Bayes Accuracy = 0.75\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  470\n",
      "Logistic Regression Accuracy = 0.76\n",
      "Naive Bayes Accuracy = 0.77\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  480\n",
      "Logistic Regression Accuracy = 0.75\n",
      "Naive Bayes Accuracy = 0.72\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "When training size is:  490\n",
      "Logistic Regression Accuracy = 0.76\n",
      "Naive Bayes Accuracy = 0.75\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEjCAYAAADDry0IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM7O8/n/wD/f6gu7xWQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x248563eadd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Block B\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import svm, datasets\n",
    "import time\n",
    "C = 1.0\n",
    "\n",
    "# Import the raw dataset\n",
    "diabetes_raw = pd.read_csv(\"diabetes.data\", header=None)\n",
    "# Drop any na values\n",
    "diabetes_cleaned = diabetes_raw.dropna(how='any')\n",
    "\n",
    "n_samples = len(diabetes_cleaned)\n",
    "graphA_y = []\n",
    "graphB_y = []\n",
    "graph_x = []\n",
    "\n",
    "# Setting the parameter for the for loop\n",
    "train_size_start = 10\n",
    "train_size_end = 500 \n",
    "interval = 10\n",
    "\n",
    "for size in range(train_size_start, train_size_end, interval):\n",
    "    # Create new Logistic Regression instance\n",
    "    lr = LogisticRegression()\n",
    "    nb = GaussianNB()\n",
    "    #svmclassifier = svm.SVC(kernel='rbf', gamma=0.7, C=C)\n",
    "    #svmclassifier = svm.SVC(kernel='linear', C=C)\n",
    "    \n",
    "    # shuffle the data directly in pandas\n",
    "    diabetes_cleaned_shuffled = shuffle(diabetes_cleaned)\n",
    "    diabetes_x = diabetes_cleaned_shuffled.iloc[:, 0:8]\n",
    "    diabetes_y = diabetes_cleaned_shuffled.iloc[:, 8]\n",
    "    \n",
    "    train_x = diabetes_x[:size]\n",
    "    train_y = diabetes_y[:size]\n",
    "    test_x = diabetes_x[500:]\n",
    "    test_y = diabetes_y[500:]\n",
    "    # Fit both models\n",
    "    \n",
    "    start = time.time()\n",
    "    lr.fit(train_x, train_y)\n",
    "    end = time.time()\n",
    "    #print(\"Logistic Regression takes: \", end-start, \" seconds\")\n",
    "    \n",
    "    start = time.time()\n",
    "    nb.fit(train_x, train_y)\n",
    "    end = time.time()\n",
    "    #print(\"Naive Bayes takes: \", end-start, \" seconds\")\n",
    "    \n",
    "    graph_x.append(size)\n",
    "    \n",
    "    \n",
    "    graphA_y.append(lr.score(test_x, test_y))\n",
    "    graphB_y.append(nb.score(test_x, test_y))\n",
    "    \n",
    "    print(\"When training size is: \", size)\n",
    "    print(\"Logistic Regression Accuracy = %.2f\" % lr.score(test_x, test_y))\n",
    "    print(\"Naive Bayes Accuracy = %.2f\" % nb.score(test_x, test_y))\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    \n",
    "# Plotting the Accuracies    \n",
    "fig = plt.figure()\n",
    "fig.suptitle('Classification Accuracies')\n",
    "graph = fig.add_subplot(111)\n",
    "graph.scatter(graph_x, graphA_y, c='b', marker=\"s\", label=\"Logistic Regression\")\n",
    "graph.scatter(graph_x, graphB_y, c='g', marker=\"s\", label=\"Naive Bayes\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:**\n",
    "\n",
    "Using data from the US Social Security Administration (https://www.ssa.gov/oact/babynames/limits.html), develop a gender classifier using only full names as features.  Compare the classifier from # 2 to this classifier. What does these new classifiers predict for the list of students in this course (for simplicity, assume that undergraduate students in this course were born in 1995, and graduate students were born in 1985)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26075</th>\n",
       "      <td>Zerek</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26076</th>\n",
       "      <td>Zhen</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26077</th>\n",
       "      <td>Ziggy</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26078</th>\n",
       "      <td>Zuberi</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26079</th>\n",
       "      <td>Zyon</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name Gender  Count\n",
       "26075   Zerek      M      5\n",
       "26076    Zhen      M      5\n",
       "26077   Ziggy      M      5\n",
       "26078  Zuberi      M      5\n",
       "26079    Zyon      M      5"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "featname = [\"Name\", \"Gender\",\"Count\"]\n",
    "names_1985_dtf = pd.read_csv(\"yob1985.txt\", header = None, names = featname)\n",
    "names_1995_dtf = pd.read_csv(\"yob1995.txt\",header = None, names = featname)\n",
    "names_1995_dtf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_name(MLA, USSAdata, alp):\n",
    "    gen_pred = []\n",
    "    for name in MLA:\n",
    "        PR_name_male = (USSAdata[(USSAdata[\"Gender\"] == 'M' ) & (USSAdata[\"Name\"] == name)].Count.sum() + alp)/(USSAdata[(USSAdata[\"Gender\"] == 'M')].Count.sum() + alp)\n",
    "        PR_name_female = (USSAdata[(USSAdata[\"Gender\"] == 'F' ) & (USSAdata[\"Name\"] == name)].Count.sum() + alp)/(USSAdata[(USSAdata[\"Gender\"] == 'F')].Count.sum() + alp)\n",
    "        PR_male = (USSAdata[(USSAdata[\"Gender\"] == 'M')].Count.sum())/(USSAdata.Count.sum() + alp)\n",
    "        PR_female = (USSAdata[(USSAdata[\"Gender\"] == 'F')].Count.sum())/(USSAdata.Count.sum() + alp )\n",
    "        gender_pr = (PR_name_male * PR_male)/(PR_name_female * PR_female + PR_name_male * PR_male)\n",
    "        if (gender_pr > 0.5):\n",
    "            #Its male\n",
    "            gen_pred.append('male')\n",
    "        else:\n",
    "            #Its female\n",
    "            gen_pred.append('female')\n",
    "    return gen_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pg_names = [\"Amal\",\n",
    "           \"Amirah\",\n",
    "           \"Devin\",\n",
    "           \"Jane\",\n",
    "           \"Nivea\",\n",
    "           \"Hyuk\",\n",
    "           \"Hanting\",\n",
    "           \"Xi\",\n",
    "           \"Yiwei\",\n",
    "            \"Zhenglin\",\n",
    "           \"Mykola\"]\n",
    "\n",
    "ug_names = [\"Philip\",\n",
    "           \"Willie\",\n",
    "           \"Ethan\",\n",
    "           \"Seamus\"]\n",
    "\n",
    "pg_gender = [\"female\",\n",
    "                \"female\",\n",
    "                \"male\",\n",
    "                \"female\",\n",
    "                \"female\",\n",
    "                \"male\",\n",
    "                \"male\",\n",
    "                \"male\",\n",
    "                \"male\",\n",
    "                \"male\",\n",
    "                \"female\"]\n",
    "\n",
    "ug_gender = [\"male\",\n",
    "                \"male\",\n",
    "              \"male\",\n",
    "                \"male\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male', 'male', 'male', 'male']\n",
      "['female', 'female', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male']\n"
     ]
    }
   ],
   "source": [
    "print(gender_pred_ug)\n",
    "print(gender_pred_pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undergraduate students:\n",
      "Philip  is predicted as  male\n",
      "Willie  is predicted as  male\n",
      "Ethan  is predicted as  male\n",
      "Seamus  is predicted as  male\n",
      "Classifier Accuracy for Undergrads: 1.0\n",
      "\n",
      "\n",
      "Graduate students:\n",
      "Amal  is predicted as  female\n",
      "Amirah  is predicted as  female\n",
      "Devin  is predicted as  male\n",
      "Jane  is predicted as  female\n",
      "Nivea  is predicted as  female\n",
      "Hyuk  is predicted as  male\n",
      "Hanting  is predicted as  male\n",
      "Xi  is predicted as  male\n",
      "Yiwei  is predicted as  male\n",
      "Zhenglin  is predicted as  male\n",
      "Mykola  is predicted as  male\n",
      "Classifier Accuracy for Grads: 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "gender_pred_ug = []\n",
    "gender_pred_pg = []\n",
    "\n",
    "gender_pred_ug = predict_name(ug_names, names_1995_dtf, alp = 1)\n",
    "gender_pred_pg = predict_name(pg_names, names_1985_dtf, alp = 1)\n",
    "\n",
    "#Predictions and Accuracy for Undergrads\n",
    "acc_ug=[]\n",
    "print(\"Undergraduate students:\")  \n",
    "for u, name in enumerate(ug_names):\n",
    "    print(name, \" is predicted as \", gender_pred_ug[u])\n",
    "    acc_ug.append(gender_pred_ug[u] == ug_gender[u])\n",
    "print(\"Classifier Accuracy for Undergrads:\",sum(acc_ug)/len(acc_ug))\n",
    "\n",
    "#Predictions and Accuracy for Grads\n",
    "acc_pg=[]\n",
    "print(\"\\n\\nGraduate students:\")\n",
    "for p, name in enumerate(pg_names):\n",
    "    print(name, \" is predicted as \", gender_pred_pg[p])\n",
    "    acc_pg.append(gender_pred_pg[p] == pg_gender[p])\n",
    "print(\"Classifier Accuracy for Grads:\",sum(acc_pg)/len(acc_pg))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33058</th>\n",
       "      <td>Zykell</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33059</th>\n",
       "      <td>Zyking</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33060</th>\n",
       "      <td>Zykir</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33061</th>\n",
       "      <td>Zyrus</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33062</th>\n",
       "      <td>Zyus</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name Gender  Count\n",
       "33058  Zykell      M      5\n",
       "33059  Zyking      M      5\n",
       "33060   Zykir      M      5\n",
       "33061   Zyrus      M      5\n",
       "33062    Zyus      M      5"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name list with different years.\n",
    "\n",
    "newnames = [\"Dylan\", \"Madison\", \"Tyler\", \"Dana\"]\n",
    "\n",
    "featname = [\"Name\", \"Gender\",\"Count\"]\n",
    "names_1988_dtf = pd.read_csv(\"yob1988.txt\", header = None, names = featname)\n",
    "names_1997_dtf = pd.read_csv(\"yob1997.txt\",header = None, names = featname)\n",
    "names_2015_dtf = pd.read_csv(\"yob2015.txt\",header = None, names = featname)\n",
    "\n",
    "names_2015_dtf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In 1988:\n",
      "Dylan  is predicted as  male\n",
      "Madison  is predicted as  female\n",
      "Tyler  is predicted as  male\n",
      "Dana  is predicted as  female\n",
      "\n",
      "In 1997:\n",
      "Dylan  is predicted as  male\n",
      "Madison  is predicted as  female\n",
      "Tyler  is predicted as  male\n",
      "Dana  is predicted as  female\n",
      "\n",
      "In 2015:\n",
      "Dylan  is predicted as  male\n",
      "Madison  is predicted as  female\n",
      "Tyler  is predicted as  male\n",
      "Dana  is predicted as  female\n"
     ]
    }
   ],
   "source": [
    "#Finding the probability of gender for years 1988, 1997 and, 2015\n",
    "gender_pred_1988 = []\n",
    "gender_pred_1997 = []\n",
    "gender_pred_2015 = []\n",
    "\n",
    "\n",
    "gender_pred_1988 = predict_name(newnames, names_1988_dtf, alp = 1)\n",
    "gender_pred_1997 = predict_name(newnames, names_1997_dtf, alp = 1)\n",
    "gender_pred_2015 = predict_name(newnames, names_2015_dtf, alp = 1)\n",
    "\n",
    "#Predictions for each year\n",
    "print(\"\\nIn 1988:\")  \n",
    "for u, name in enumerate(newnames):\n",
    "    print(name, \" is predicted as \", gender_pred_1988[u])\n",
    "    \n",
    "print(\"\\nIn 1997:\")  \n",
    "for u, name in enumerate(newnames):\n",
    "    print(name, \" is predicted as \", gender_pred_1988[u])\n",
    "    \n",
    "print(\"\\nIn 2015:\")  \n",
    "for u, name in enumerate(newnames):\n",
    "    print(name, \" is predicted as \", gender_pred_1988[u])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:**\n",
    "\n",
    "Do the tutorial at http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html  \n",
    "a.\tExplain the “curse of dimensionality” in your own words and describe how this problem is addressed by \n",
    "\n",
    "i.\tk-Nearest-Neighbor  \n",
    "ii.\tMaxEnt  \n",
    "iii.\tNaïve Bayes, and   \n",
    "iv.\tSupport Vector classifiers   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target\n",
    "np.unique(iris_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0, 0, 0, 2, 1, 2, 0])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split iris data in train and test data\n",
    "# A random permutation, to split the data randomly\n",
    "np.random.seed(0)\n",
    "indices = np.random.permutation(len(iris_X))\n",
    "iris_X_train = iris_X[indices[:-10]]\n",
    "iris_y_train = iris_y[indices[:-10]]\n",
    "iris_X_test  = iris_X[indices[-10:]]\n",
    "iris_y_test  = iris_y[indices[-10:]]\n",
    "# Create and fit a nearest-neighbor classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(iris_X_train, iris_y_train) \n",
    "\n",
    "knn.predict(iris_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0, 2, 1, 2, 0])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:**\n",
    "\n",
    "Do the example assignment at http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html#example-svm-plot-separating-hyperplane-py\n",
    "\n",
    "a.\tComment each line of code, explaining its function\n",
    "\n",
    "b.\tExplain the concept of a maximum-margin hyperplane in your own words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEVCAr4QQhSJ/x/rlu7KUxNegAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24857954358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "#Importing relevant libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "# we create 40 separable points\n",
    "X, y = make_blobs(n_samples=40, centers=2, random_state=6)\n",
    "\n",
    "# Fit the model, no regularization done.\n",
    "clf = svm.SVC(kernel='linear', C=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "\n",
    "# plot the decision function\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30) #Returns evenly spaced numbers over a specified interval.\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)           #This returns matrices from vectors.\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape) #Distance of the samples X from the separating hyperplane and reshapes it to the shape of XX.\n",
    "\n",
    "# plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "# plot support vectors\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Question 7:**\n",
    "\n",
    "Use a linear SVM to construct a sentiment classifier using the Breast Cancer Wisconsin (Diagnostic) Data Set from the UCI Repository\n",
    "\n",
    "a.\tInclude all features and use them to construct a soft-margin linear SVM classifier using a standard hinge loss function. Using 10-fold cross-validation, what is this classifier’s accuracy? \n",
    "\n",
    "b.\tExplain a hinge loss function in your own words\n",
    "\n",
    "c.\tUse a non-linear kernel (rbf,poly) to generate classification results. What is this classifier’s accuracy? Is there another metric, besides accuracy, that would lead you to favor the classifier with the lower accuracy? (f1, precision, recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>M_radius</th>\n",
       "      <th>SE_radius</th>\n",
       "      <th>W_radius</th>\n",
       "      <th>M_texture</th>\n",
       "      <th>SE_texture</th>\n",
       "      <th>W_texture</th>\n",
       "      <th>M_perimeter</th>\n",
       "      <th>SE_perimeter</th>\n",
       "      <th>...</th>\n",
       "      <th>W_concavity</th>\n",
       "      <th>M_concave points</th>\n",
       "      <th>SE_concave points</th>\n",
       "      <th>W_concave points</th>\n",
       "      <th>M_symmetry</th>\n",
       "      <th>SE_symmetry</th>\n",
       "      <th>W_symmetry</th>\n",
       "      <th>M_fract dimension</th>\n",
       "      <th>SE_fract dimension</th>\n",
       "      <th>W_fract dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Diagnosis  M_radius  SE_radius  W_radius  M_texture  SE_texture  \\\n",
       "0    842302         M     17.99      10.38    122.80     1001.0     0.11840   \n",
       "1    842517         M     20.57      17.77    132.90     1326.0     0.08474   \n",
       "2  84300903         M     19.69      21.25    130.00     1203.0     0.10960   \n",
       "3  84348301         M     11.42      20.38     77.58      386.1     0.14250   \n",
       "4  84358402         M     20.29      14.34    135.10     1297.0     0.10030   \n",
       "\n",
       "   W_texture  M_perimeter  SE_perimeter        ...          W_concavity  \\\n",
       "0    0.27760       0.3001       0.14710        ...                25.38   \n",
       "1    0.07864       0.0869       0.07017        ...                24.99   \n",
       "2    0.15990       0.1974       0.12790        ...                23.57   \n",
       "3    0.28390       0.2414       0.10520        ...                14.91   \n",
       "4    0.13280       0.1980       0.10430        ...                22.54   \n",
       "\n",
       "   M_concave points  SE_concave points  W_concave points  M_symmetry  \\\n",
       "0             17.33             184.60            2019.0      0.1622   \n",
       "1             23.41             158.80            1956.0      0.1238   \n",
       "2             25.53             152.50            1709.0      0.1444   \n",
       "3             26.50              98.87             567.7      0.2098   \n",
       "4             16.67             152.20            1575.0      0.1374   \n",
       "\n",
       "   SE_symmetry  W_symmetry  M_fract dimension  SE_fract dimension  \\\n",
       "0       0.6656      0.7119             0.2654              0.4601   \n",
       "1       0.1866      0.2416             0.1860              0.2750   \n",
       "2       0.4245      0.4504             0.2430              0.3613   \n",
       "3       0.8663      0.6869             0.2575              0.6638   \n",
       "4       0.2050      0.4000             0.1625              0.2364   \n",
       "\n",
       "   W_fract dimension  \n",
       "0            0.11890  \n",
       "1            0.08902  \n",
       "2            0.08758  \n",
       "3            0.17300  \n",
       "4            0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols = [\"ID\", \"Diagnosis\", \n",
    "       \"M_radius\", \"SE_radius\", \"W_radius\",\n",
    "      \"M_texture\", \"SE_texture\", \"W_texture\",\n",
    "      \"M_perimeter\", \"SE_perimeter\", \"W_perimeter\",\n",
    "      \"M_area\", \"SE_area\", \"W_area\",\n",
    "      \"M_smoothness\", \"SE_smoothness\", \"W_smoothness\",\n",
    "      \"M_compactness\", \"SE_compactness\", \"W_compactness\",\n",
    "      \"M_concavity\", \"SE_concavity\", \"W_concavity\",\n",
    "      \"M_concave points\", \"SE_concave points\", \"W_concave points\",\n",
    "      \"M_symmetry\", \"SE_symmetry\", \"W_symmetry\",\n",
    "      \"M_fract dimension\", \"SE_fract dimension\", \"W_fract dimension\"]\n",
    "wdbc = pd.read_csv(\"wdbc.data.txt\", header=None, names=cols)\n",
    "wdbc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>M_radius</th>\n",
       "      <th>SE_radius</th>\n",
       "      <th>W_radius</th>\n",
       "      <th>M_texture</th>\n",
       "      <th>SE_texture</th>\n",
       "      <th>W_texture</th>\n",
       "      <th>M_perimeter</th>\n",
       "      <th>SE_perimeter</th>\n",
       "      <th>...</th>\n",
       "      <th>W_concavity</th>\n",
       "      <th>M_concave points</th>\n",
       "      <th>SE_concave points</th>\n",
       "      <th>W_concave points</th>\n",
       "      <th>M_symmetry</th>\n",
       "      <th>SE_symmetry</th>\n",
       "      <th>W_symmetry</th>\n",
       "      <th>M_fract dimension</th>\n",
       "      <th>SE_fract dimension</th>\n",
       "      <th>W_fract dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Diagnosis  M_radius  SE_radius  W_radius  M_texture  SE_texture  \\\n",
       "0    842302          1     17.99      10.38    122.80     1001.0     0.11840   \n",
       "1    842517          1     20.57      17.77    132.90     1326.0     0.08474   \n",
       "2  84300903          1     19.69      21.25    130.00     1203.0     0.10960   \n",
       "3  84348301          1     11.42      20.38     77.58      386.1     0.14250   \n",
       "4  84358402          1     20.29      14.34    135.10     1297.0     0.10030   \n",
       "\n",
       "   W_texture  M_perimeter  SE_perimeter        ...          W_concavity  \\\n",
       "0    0.27760       0.3001       0.14710        ...                25.38   \n",
       "1    0.07864       0.0869       0.07017        ...                24.99   \n",
       "2    0.15990       0.1974       0.12790        ...                23.57   \n",
       "3    0.28390       0.2414       0.10520        ...                14.91   \n",
       "4    0.13280       0.1980       0.10430        ...                22.54   \n",
       "\n",
       "   M_concave points  SE_concave points  W_concave points  M_symmetry  \\\n",
       "0             17.33             184.60            2019.0      0.1622   \n",
       "1             23.41             158.80            1956.0      0.1238   \n",
       "2             25.53             152.50            1709.0      0.1444   \n",
       "3             26.50              98.87             567.7      0.2098   \n",
       "4             16.67             152.20            1575.0      0.1374   \n",
       "\n",
       "   SE_symmetry  W_symmetry  M_fract dimension  SE_fract dimension  \\\n",
       "0       0.6656      0.7119             0.2654              0.4601   \n",
       "1       0.1866      0.2416             0.1860              0.2750   \n",
       "2       0.4245      0.4504             0.2430              0.3613   \n",
       "3       0.8663      0.6869             0.2575              0.6638   \n",
       "4       0.2050      0.4000             0.1625              0.2364   \n",
       "\n",
       "   W_fract dimension  \n",
       "0            0.11890  \n",
       "1            0.08902  \n",
       "2            0.08758  \n",
       "3            0.17300  \n",
       "4            0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdbc.Diagnosis.unique()\n",
    "\n",
    "wdbc['Diagnosis'] = wdbc['Diagnosis'].map({'M':1,'B':0})\n",
    "wdbc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvSR1nEEvSR1n0EtSx/1/2SiZRd9xKYYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23e9c395908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wdbc.describe()\n",
    "plt.hist(wdbc['Diagnosis'])\n",
    "plt.title('Diagnosis (M=1 , B=0)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_mean=list(wdbc.columns[1:11])\n",
    "# split dataframe into two based on diagnosis\n",
    "wdbcM=wdbc[wdbc['Diagnosis'] ==1]\n",
    "wdbcB=wdbc[wdbc['Diagnosis'] ==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nivea\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABQ0AAALNCAYAAACFy0ejAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23e9c92d3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Showing the correlation of the data using plots\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(18,10))\n",
    "axes = axes.ravel()\n",
    "for idx,ax in enumerate(axes):\n",
    "    ax.figure\n",
    "    binwidth= (max(wdbc[features_mean[idx]]) - min(wdbc[features_mean[idx]]))/50\n",
    "    ax.hist([wdbcM[features_mean[idx]],wdbcB[features_mean[idx]]], bins=np.arange(min(wdbc[features_mean[idx]]), max(wdbc[features_mean[idx]]) + binwidth, binwidth) , alpha=0.5,stacked=True, normed = True, label=['M','B'],color=['r','g'])\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title(features_mean[idx])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 31)\n"
     ]
    }
   ],
   "source": [
    "#Assigning Target and Data\n",
    "datacols = [col for col in wdbc.columns if col not in ['Diagnosis']]\n",
    "X = wdbc[datacols].values\n",
    "Y = wdbc[\"Diagnosis\"].values\n",
    "print(X.shape)\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Splitting into 70:30 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "C = 1.0\n",
    "#clf_wdbc = LinearSVC(loss = 'hinge')\n",
    "clf_wdbc = svm.SVC(kernel='rbf')\n",
    "#clf_wdbc = BernoulliNB()\n",
    "#clf_wdbc = LogisticRegression()\n",
    "#clf_wdbc = svm.SVC(kernel='poly', decision_function_shape='ovo')\n",
    "#clf_wdbc = svm.SVC(kernel='sigmoid', C=C)\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "#Fit the model:\n",
    "clf_wdbc.fit(X_train, y_train) \n",
    "\n",
    "#Predict the outcome\n",
    "Predicted_Y = clf_wdbc.predict(X_test)\n",
    "\n",
    "#Print accuracy\n",
    "accuracy = accuracy_score(Predicted_Y, y_test)\n",
    "print(\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "scr = precision_recall_fscore_support(y_test, Predicted_Y, average='binary')\n",
    "print(\"Scores:\", scr[0])\n",
    "\n",
    "cv = KFold(n_splits=10)\n",
    "scores = []\n",
    "accuracy_res = []\n",
    "\n",
    "for i, (train_wdbc, test_wdbc) in enumerate(cv.split(X)):\n",
    "    #print(\"test size, train size\",len(test_wdbc), len(train_wdbc))\n",
    "    clf_wdbc.fit(X[train_wdbc], Y[train_wdbc])\n",
    "    Predicted_Y = clf_wdbc.predict(X[test_wdbc])\n",
    "    #print(Predicted_Y)\n",
    "    for j in range(Predicted_Y.shape[0]):\n",
    "        if(Predicted_Y[j]<0.5):\n",
    "            Predicted_Y[j] = 0\n",
    "        else :\n",
    "            Predicted_Y[j] = 1\n",
    "    print(\"Accuracy of \",i+1,\" trial :\", accuracy_score(Predicted_Y, Y[test_wdbc]))\n",
    "    accuracy_res.append(accuracy_score(Predicted_Y, Y[test_wdbc]))\n",
    "    print(\"Lengths:\\n\",Predicted_Y)\n",
    "    print(Y[test_wdbc])\n",
    "    scores.append(precision_recall_fscore_support(Y[test_wdbc], Predicted_Y, average='binary'))\n",
    "    print(\"Scores:\", scores[i])\n",
    "print(\"\\nAverage Accuracy:\", np.mean(accuracy_res))\n",
    "print(\"Average Precision:\",np.mean(list(zip(*scores))[0]))\n",
    "print(\"Average Recall:\",np.mean(list(zip(*scores))[1]))\n",
    "print(\"Average Fscore:\",np.mean(list(zip(*scores))[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:**\n",
    "\n",
    "Linear SVM Average Accuracy: 0.646961152882  <br>\n",
    "Average Precision: 0.516895604396\n",
    "Average Recall: 0.334216779597\n",
    "Average Fscore: 0.286517069133 <br>\n",
    "\n",
    "RBF Average Accuracy: 0.627662907268 <br>\n",
    "\n",
    "Poly Average Accuracy: 0.578960983211 <br> For 2 splits (since the processing power of my laptop is really low)\n",
    "\n",
    "Average Precision: 0.50877192982456143\n",
    "Average Recall: 1.0\n",
    "Average Fscore: 0.67441860465116277 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8:**\n",
    "Do the example classifications at http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html\n",
    "\n",
    "a.\tComment each line of code, explaining its function\n",
    "\n",
    "b.\tExplain the results of each of these outputs. How do they differ and why?\n",
    "\n",
    "c.\tExplain the “kernel trick” in your own words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEFCAYAAAAxAZr2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24856d3a208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):  #Function defined for the purpose of creating a grid\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "# Take the first two features. We could avoid this by using a two-dim dataset\n",
    "X = iris.data[:, :2]  #The data considers only the first two features - sepal length and width\n",
    "y = iris.target\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 1.0  # SVM regularization parameter\n",
    "models = (svm.SVC(kernel='linear', C=C),\n",
    "          svm.LinearSVC(C=C),\n",
    "          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "          svm.SVC(kernel='poly', degree=3, C=C))\n",
    "models = (clf.fit(X, y) for clf in models)\n",
    "\n",
    "# title for the plots\n",
    "titles = ('SVC with linear kernel',\n",
    "          'LinearSVC (linear kernel)',\n",
    "          'SVC with RBF kernel',\n",
    "          'SVC with polynomial (degree 3) kernel')\n",
    "\n",
    "# Set-up 2x2 grid for plotting.\n",
    "fig, sub = plt.subplots(2, 2)\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "for clf, title, ax in zip(models, titles, sub.flatten()):\n",
    "    plot_contours(ax, clf, xx, yy,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel('Sepal length')\n",
    "    ax.set_ylabel('Sepal width')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10:**\n",
    "\n",
    "EXTRA CREDIT: Using all of the classifier types that you’ve learned so far (MaxEnt, Naïve Bayes, SVM), create sentiment classifiers using the UCI Sentiment Labeled Sentences dataset. Report their respective accuracy scores. Which classifier would you use for this task and why? (HINT: Performance of the Naïve Bayes classifier can be boosted using TF-IDF weighting) (positive/negative review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0                           Wow... Loved this place.          1\n",
       "1                                 Crust is not good.          0\n",
       "2          Not tasty and the texture was just nasty.          0\n",
       "3  Stopped by during the late May bank holiday of...          1\n",
       "4  The selection on the menu was great and so wer...          1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# load yelp reviews dataset with feature selection\n",
    "df_sentiment = pd.read_csv('yelp_labelled.txt', sep =\"\\t\", names= ['text','sentiment'])\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "#Converting text into Numpy arrays\n",
    "X = df_sentiment[\"text\"].values\n",
    "Y = df_sentiment[\"sentiment\"].values\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 100 900 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Splitting into 90:10 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF-IDF transformation for train set\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "sentiment_tf = TfidfVectorizer()\n",
    "X_train_tf = sentiment_tf.fit_transform(X_train)\n",
    "#X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF_IDF tranform method applied to the test data\n",
    "X_test_tf = sentiment_tf.transform(X_test)\n",
    "#X_test_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the classifiers and fitting the data\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "bnb = BernoulliNB().fit(X_train_tf, y_train)\n",
    "lr = LogisticRegression(C=1.0, penalty='l2').fit(X_train_tf, y_train)\n",
    "svc_lin = svm.SVC(kernel=\"linear\", C=1.0, gamma=\"auto\").fit(X_train_tf, y_train)\n",
    "svc_rbf = svm.SVC(kernel=\"rbf\", C=1.0, gamma=\"auto\").fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes : \n",
      "Accuracy: 0.74\n",
      "Scores: \n",
      "Precision: 0.754518705338\n",
      "Recall: 0.742296918768\n",
      "Fscore: 0.737373737374\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Logistic Regression : \n",
      "Accuracy: 0.84\n",
      "Scores: \n",
      "Precision: 0.841830590124\n",
      "Recall: 0.840736294518\n",
      "Fscore: 0.83993597439\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "SVM - Linear  : \n",
      "Accuracy: 0.84\n",
      "Scores: \n",
      "Precision: 0.844444444444\n",
      "Recall: 0.841136454582\n",
      "Fscore: 0.839743589744\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "SVM - RBF : \n",
      "Accuracy: 0.49\n",
      "Scores: \n",
      "Precision: 0.245\n",
      "Recall: 0.5\n",
      "Fscore: 0.328859060403\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nivea\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "scr_bnb =[]\n",
    "pred_y_bnb = bnb.predict(X_test_tf)\n",
    "#print(pred_y_bnb)\n",
    "print(\"Bernoulli Naive Bayes : \\nAccuracy:\", accuracy_score(pred_y_bnb, y_test))\n",
    "scr_bnb = precision_recall_fscore_support(y_test, pred_y_bnb)\n",
    "print(\"Scores: \")\n",
    "print(\"Precision:\",np.mean(scr_bnb[0]))\n",
    "print(\"Recall:\",np.mean(scr_bnb[1]))\n",
    "print(\"Fscore:\",np.mean(scr_bnb[2]))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\n",
    "scr_lr =[]\n",
    "pred_y_lr = lr.predict(X_test_tf)\n",
    "print(\"Logistic Regression : \\nAccuracy:\", accuracy_score(pred_y_lr, y_test))\n",
    "scr_lr = precision_recall_fscore_support(y_test, pred_y_lr)\n",
    "print(\"Scores: \")\n",
    "print(\"Precision:\",np.mean(scr_lr[0]))\n",
    "print(\"Recall:\",np.mean(scr_lr[1]))\n",
    "print(\"Fscore:\",np.mean(scr_lr[2]))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\n",
    "scr_lin =[]\n",
    "pred_y_svclin = svc_lin.predict(X_test_tf)\n",
    "print(\"SVM - Linear  : \\nAccuracy:\", accuracy_score(pred_y_svclin, y_test))\n",
    "scr_lin = precision_recall_fscore_support(y_test, pred_y_svclin)\n",
    "print(\"Scores: \")\n",
    "print(\"Precision:\",np.mean(scr_lin[0]))\n",
    "print(\"Recall:\",np.mean(scr_lin[1]))\n",
    "print(\"Fscore:\",np.mean(scr_lin[2]))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\n",
    "scr_rbf =[]\n",
    "pred_y_rbf = svc_rbf.predict(X_test_tf)\n",
    "print(\"SVM - RBF : \\nAccuracy:\", accuracy_score(pred_y_rbf, y_test))\n",
    "scr_rbf = precision_recall_fscore_support(y_test, pred_y_rbf)\n",
    "print(\"Scores: \")\n",
    "print(\"Precision:\",np.mean(scr_rbf[0]))\n",
    "print(\"Recall:\",np.mean(scr_rbf[1]))\n",
    "print(\"Fscore:\",np.mean(scr_rbf[2]))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'the presentation of the food was awful.' => 0\n",
      "\"Worst food/service I've had in a while.\" => 0\n",
      "'Never again will I be dining at this place!' => 1\n",
      "'I guess maybe we went on an off night but it was disgraceful.' => 1\n",
      "'As a sushi lover avoid this place by all means.' => 1\n",
      "\"The ambiance isn't much better.\" => 1\n",
      "'This hole in the wall has great Mexican street tacos, and friendly staff.' => 0\n",
      "\"If the food isn't bad enough for you, then enjoy dealing with the world's worst/annoying drunk people.\" => 1\n",
      "'Will never, ever go back.' => 1\n",
      "'The atmosphere here is fun.' => 0\n",
      "'The pancake was also really good and pretty large at that.' => 0\n",
      "'All of the tapas dishes were delicious!' => 0\n",
      "\"The chains, which I'm no fan of, beat this place easily.\" => 0\n",
      "'Everyone is very attentive, providing excellent customer service.' => 0\n",
      "'The staff are also very friendly and efficient.' => 0\n",
      "'Loved it...friendly servers, great food, wonderful and imaginative menu.' => 0\n",
      "'After the disappointing dinner we went elsewhere for dessert.' => 1\n",
      "\"Perhaps I caught them on an off night judging by the other reviews, but I'm not inspired to go back.\" => 1\n",
      "'I was disgusted because I was pretty sure that was human hair.' => 1\n",
      "'The sweet potato fries were very good and seasoned well.' => 0\n",
      "'This was like the final blow!' => 0\n",
      "\"This place is a jewel in Las Vegas, and exactly what I've been hoping to find in nearly ten years living here.\" => 1\n",
      "'Fantastic food!' => 0\n",
      "'She ordered a toasted English muffin that came out untoasted.' => 1\n",
      "'I ordered Albondigas soup - which was just warm - and tasted like tomato soup with frozen meatballs.' => 0\n",
      "'The staff is super nice and very quick even with the crazy crowds of the downtown juries, lawyers, and court staff.' => 0\n",
      "'The only thing I did like was the prime rib and dessert section.' => 0\n",
      "'I LOVED their mussels cooked in this wine reduction, the duck was tender, and their potato dishes were delicious.' => 0\n",
      "'Food was good, service was good, Prices were good.' => 0\n",
      "'Crust is not good.' => 0\n",
      "'The crêpe was delicate and thin and moist.' => 0\n",
      "'That said, our mouths and bellies were still quite pleased.' => 0\n",
      "'An absolute must visit!' => 0\n",
      "'When I received my Pita it was huge it did have a lot of meat in it so thumbs up there.' => 1\n",
      "'Service was fantastic.' => 0\n",
      "\"Friend's pasta -- also bad, he barely touched it.\" => 1\n",
      "'The real disappointment was our waiter.' => 1\n",
      "'My wife had the Lobster Bisque soup which was lukewarm.' => 0\n",
      "'They brought a fresh batch of fries and I was thinking yay something warm but no!' => 0\n",
      "'Thus far, have only visited twice and the food was absolutely delicious each time.' => 0\n",
      "'This wonderful experience made this place a must-stop whenever we are in town again.' => 0\n",
      "'Appetite instantly gone.' => 0\n",
      "'We were promptly greeted and seated.' => 0\n",
      "'Needless to say, we will never be back here again.' => 1\n",
      "'There is nothing privileged about working/eating there.' => 1\n",
      "'Bland and flavorless is a good way of describing the barely tepid meat.' => 1\n",
      "'This place should honestly be blown up.' => 1\n",
      "\"The waiter wasn't helpful or friendly and rarely checked on us.\" => 0\n",
      "\"By this point, my friends and I had basically figured out this place was a joke and didn't mind making it publicly and loudly known.\" => 1\n",
      "'I hate those things as much as cheap quality black olives.' => 1\n",
      "'Both of them were truly unbelievably good, and I am so glad we went back.' => 0\n",
      "'You get incredibly fresh fish, prepared with care.' => 0\n",
      "'The restaurant atmosphere was exquisite.' => 0\n",
      "'Service is quick and friendly.' => 0\n",
      "'Hot dishes are not hot, cold dishes are close to room temp.I watched staff prepare food with BARE HANDS, no gloves.Everything is deep fried in oil.' => 1\n",
      "'If you look for authentic Thai food, go else where.' => 1\n",
      "\"You can't beat that.\" => 0\n",
      "'The Veggitarian platter is out of this world!' => 0\n",
      "'It lacked flavor, seemed undercooked, and dry.' => 1\n",
      "'All the bread is made in-house!' => 0\n",
      "'I consider this theft.' => 0\n",
      "\"Maybe if they weren't cold they would have been somewhat edible.\" => 1\n",
      "'My husband and I ate lunch here and were very disappointed with the food and service.' => 0\n",
      "\"The warm beer didn't help.\" => 0\n",
      "\"I can assure you that you won't be disappointed.\" => 1\n",
      "'The live music on Fridays totally blows.' => 0\n",
      "'The seafood was fresh and generous in portion.' => 0\n",
      "'This place is two thumbs up....way up.' => 1\n",
      "'The Greek dressing was very creamy and flavorful.' => 0\n",
      "'I swung in to give them a try but was deeply disappointed.' => 1\n",
      "'I love the decor with the Chinese calligraphy wall paper.' => 0\n",
      "'The food was excellent and service was very good.' => 0\n",
      "'The restaurant is very clean and has a family restaurant feel to it.' => 0\n",
      "'Much better than the other AYCE sushi place I went to in Vegas.' => 1\n",
      "'It was so bad, I had lost the heart to finish it.' => 1\n",
      "'For service, I give them no stars.' => 1\n",
      "'Def coming back to bowl next time' => 1\n",
      "'The chips and salsa were really good, the salsa was very fresh.' => 0\n",
      "'We got sitting fairly fast, but, ended up waiting 40 minutes just to place our order, another 30 minutes before the food arrived.' => 1\n",
      "'The patio seating was very comfortable.' => 0\n",
      "'I have been in more than a few bars in Vegas, and do not ever recall being charged for tap water.' => 1\n",
      "'The food was very good and I enjoyed every mouthful, an enjoyable relaxed venue for couples small family groups etc.' => 0\n",
      "\"I will come back here every time I'm in Vegas.\" => 0\n",
      "'Food was below average.' => 1\n",
      "'This is my new fav Vegas buffet spot.' => 0\n",
      "'The sergeant pepper beef sandwich with auju sauce is an excellent sandwich as well.' => 0\n",
      "'The descriptions said \"yum yum sauce\" and another said \"eel sauce\", yet another said \"spicy mayo\"...well NONE of the rolls had sauces on them.' => 0\n",
      "\"I'd rather eat airline food, seriously.\" => 0\n",
      "'This was my first crawfish experience, and it was delicious!' => 0\n",
      "'Weird vibe from owners.' => 0\n",
      "'I am far from a sushi connoisseur but I can definitely tell the difference between good food and bad food and this was certainly bad food.' => 1\n",
      "'Their daily specials are always a hit with my group.' => 0\n",
      "'The RI style calamari was a joke.' => 1\n",
      "'I had the mac salad and it was pretty bland so I will not be getting that again.' => 1\n",
      "'After waiting an hour and being seated, I was not in the greatest of moods.' => 1\n",
      "\"This isn't a small family restaurant, this is a fine dining establishment.\" => 0\n",
      "'AN HOUR... seriously?' => 0\n",
      "'Will go back next trip out.' => 1\n",
      "'Would come back again if I had a sushi craving while in Vegas.' => 0\n",
      "'This place has a lot of promise but fails to deliver.' => 0\n"
     ]
    }
   ],
   "source": [
    "for d, emo in zip(X_test, pred_y_bnb):\n",
    "    print('%r => %s' % (d, Y[emo]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Great place' => 0\n",
      "'Good ambience and good food' => 0\n",
      "'Love it' => 0\n",
      "'Rude waiters' => 1\n"
     ]
    }
   ],
   "source": [
    "newdata = [\"Great place\", \"Good ambience and good food\", \"Love it\", \"Rude waiters\"]\n",
    "\n",
    "X_new_tf = sentiment_tf.transform(newdata)\n",
    "\n",
    "predicted = bnb.predict(X_new_tf)\n",
    "\n",
    "for doc, sentiment in zip(newdata, predicted):\n",
    "    print('%r => %s' % (doc, Y[sentiment]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11:**\n",
    "\n",
    "Fit a MaxEnt (ME) model, a Naïve Bayes (NB) model, and a Support Vector classifier model to the Thoracic Surgery Data dataset, which aims to predict 1-year survival rate of thoracic surgery patients. <br>\n",
    "a.\tCalculate the baseline proportion of the population that survives for greater than one year. If necessary, undersample (sklearn undersample package) the population such that there are equal numbers of survivors and non-survivors.<br>\n",
    "b.\tSelect a model scoring criterion and justify your choice in the context of the dataset. (use one of the metrics like precision, recall etc) <br>\n",
    "c.\tUse grid search cross validation to select the best-fitting classifier for each type of model. What is the best fitting model in each category, and what is its score?  <br>\n",
    "d.\tAfter applying 10-fold cross-validation, generate an averaged contingency table for each best-fitting classifier in the ME, Bernoulli NB, and SVC categories.  <br>\n",
    "e.\tFor each best-fitting classifier, indicate the following:\n",
    "i.\tAccuracy\n",
    "ii.\tPrecision\n",
    "iii.\tRecall\n",
    "iv.\tF1 <br>\n",
    "f.\tFor each best-fitting classifier generate a plot of the ROC curve and indicate the AUC <br>\n",
    "g.\tMake an argument for the selection of a classifier based on the metrics listed above; however, do not base your argument solely on the metric selected for part a, and justify your choice. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DGN</th>\n",
       "      <th>PRE4</th>\n",
       "      <th>PRE5</th>\n",
       "      <th>PRE6</th>\n",
       "      <th>PRE7</th>\n",
       "      <th>PRE8</th>\n",
       "      <th>PRE9</th>\n",
       "      <th>PRE10</th>\n",
       "      <th>PRE11</th>\n",
       "      <th>PRE14</th>\n",
       "      <th>PRE17</th>\n",
       "      <th>PRE19</th>\n",
       "      <th>PRE25</th>\n",
       "      <th>PRE30</th>\n",
       "      <th>PRE32</th>\n",
       "      <th>AGE</th>\n",
       "      <th>Risk1Yr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DGN2</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.16</td>\n",
       "      <td>PRZ1</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>OC14</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DGN3</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.88</td>\n",
       "      <td>PRZ0</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>OC12</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>51</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DGN3</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.08</td>\n",
       "      <td>PRZ1</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>OC11</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>59</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DGN3</td>\n",
       "      <td>3.68</td>\n",
       "      <td>3.04</td>\n",
       "      <td>PRZ0</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>OC11</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>54</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DGN3</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.96</td>\n",
       "      <td>PRZ2</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>OC11</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>73</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DGN  PRE4  PRE5  PRE6 PRE7 PRE8 PRE9 PRE10 PRE11 PRE14 PRE17 PRE19 PRE25  \\\n",
       "0  DGN2  2.88  2.16  PRZ1    F    F    F     T     T  OC14     F     F     F   \n",
       "1  DGN3  3.40  1.88  PRZ0    F    F    F     F     F  OC12     F     F     F   \n",
       "2  DGN3  2.76  2.08  PRZ1    F    F    F     T     F  OC11     F     F     F   \n",
       "3  DGN3  3.68  3.04  PRZ0    F    F    F     F     F  OC11     F     F     F   \n",
       "4  DGN3  2.44  0.96  PRZ2    F    T    F     T     T  OC11     F     F     F   \n",
       "\n",
       "  PRE30 PRE32  AGE Risk1Yr  \n",
       "0     T     F   60       F  \n",
       "1     T     F   51       F  \n",
       "2     T     F   59       F  \n",
       "3     F     F   54       F  \n",
       "4     T     F   73       T  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "colnames = [\"DGN\",\"PRE4\",\"PRE5\",\"PRE6\",\"PRE7\",\"PRE8\",\"PRE9\",\"PRE10\",\"PRE11\",\"PRE14\"\n",
    "            ,\"PRE17\",\"PRE19\",\"PRE25\",\"PRE30\",\"PRE32\",\"AGE\",\"Risk1Yr\"]\n",
    "\n",
    "ts_df = pd.read_csv(\"thoracicsurgery.txt\", names = colnames)\n",
    "\n",
    "ts_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survivors: 70\n",
      "Non-Survivors: 400\n",
      "Probabilities:\n",
      " F    0.851064\n",
      "T    0.148936\n",
      "Name: Risk1Yr, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Finding the survivors and non survivors \n",
    "import numpy as np\n",
    "\n",
    "survivors = ts_df[ts_df['Risk1Yr'] == 'T']\n",
    "print(\"Survivors:\",len(survivors))\n",
    "non_survivors =  ts_df[ts_df['Risk1Yr'] == 'F']\n",
    "print(\"Non-Survivors:\",len(non_survivors))\n",
    "print(\"Probabilities:\\n\",ts_df.Risk1Yr.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[0 1]\n",
      "Number of Survivors: 400\n",
      "Number of Non-Survivors: 70\n"
     ]
    }
   ],
   "source": [
    "#Separating the numerical and categorical values\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "categorical_ts = [0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 ,13 ,14]\n",
    "print(categorical_ts)\n",
    "continuous_ts = [1,2,15]\n",
    "\n",
    "#Seperating data and target\n",
    "datacols = [col for col in ts_df.columns if col not in ['Risk1Yr']]\n",
    "            \n",
    "X = ts_df[datacols].values\n",
    "\n",
    "y = ts_df[\"Risk1Yr\"].values\n",
    "\n",
    "lenc_x= LabelEncoder()\n",
    "for c in categorical_ts:\n",
    "    X[:,c] = lenc_x.fit_transform(X[:,c]) \n",
    "ohc = OneHotEncoder(categorical_features = categorical_ts)\n",
    "X = ohc.fit_transform(X).toarray()\n",
    "\n",
    "# Encoding the Target\n",
    "lenc_y = LabelEncoder()\n",
    "y = lenc_y.fit_transform(y)\n",
    "\n",
    "# Feature Scaling\n",
    "ssc_x = StandardScaler()\n",
    "for c in continuous_ts:\n",
    "    X[:,c:] = ssc_x.fit_transform(X[:,c:])\n",
    "    \n",
    "print(np.unique(y))\n",
    "print(\"Number of Survivors:\",np.bincount(y)[0])\n",
    "print(\"Number of Non-Survivors:\",np.bincount(y)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "Number of Survivors: 70\n",
      "Number of Non-Survivors: 70\n"
     ]
    }
   ],
   "source": [
    "# Under-sampling the dataset to render balanced classes\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "undersamp = RandomUnderSampler(random_state=0)\n",
    "X, y = undersamp.fit_sample(X, y)\n",
    "\n",
    "print(np.unique(y))\n",
    "print(\"Number of Survivors:\",np.bincount(y)[0])\n",
    "print(\"Number of Non-Survivors:\",np.bincount(y)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression Accuracy\n",
      "1 Trial Accuracy:  0.5\n",
      "2 Trial Accuracy:  0.5714285714285714\n",
      "3 Trial Accuracy:  0.7142857142857143\n",
      "4 Trial Accuracy:  0.42857142857142855\n",
      "5 Trial Accuracy:  0.42857142857142855\n",
      "6 Trial Accuracy:  0.35714285714285715\n",
      "7 Trial Accuracy:  0.5\n",
      "8 Trial Accuracy:  0.35714285714285715\n",
      "9 Trial Accuracy:  0.42857142857142855\n",
      "10 Trial Accuracy:  0.5\n",
      "\n",
      "\n",
      "Avg. Accuracy for Logistic Regression:  0.47857142857142854\n",
      "Confusion Matrix for Logistic Regression: \n",
      " [[37. 33.]\n",
      " [40. 30.]]\n",
      "\n",
      " Bernoulli Naive Bayes Accuracy\n",
      "1 Trial Accuracy:  0.42857142857142855\n",
      "2 Trial Accuracy:  0.6428571428571429\n",
      "3 Trial Accuracy:  0.5714285714285714\n",
      "4 Trial Accuracy:  0.5714285714285714\n",
      "5 Trial Accuracy:  0.5\n",
      "6 Trial Accuracy:  0.35714285714285715\n",
      "7 Trial Accuracy:  0.6428571428571429\n",
      "8 Trial Accuracy:  0.35714285714285715\n",
      "9 Trial Accuracy:  0.5714285714285714\n",
      "10 Trial Accuracy:  0.5714285714285714\n",
      "\n",
      "\n",
      "Avg. Accuracy for Bernoulli Naive Bayes:  0.5214285714285714\n",
      "Confusion Matrix for Bernoulli Naive Bayes: \n",
      " [[38. 32.]\n",
      " [35. 35.]]\n",
      "\n",
      " SVC - Linear Accuracy\n",
      "(126, 37)\n",
      "(126,)\n",
      "1 Trial Accuracy:  0.42857142857142855\n",
      "2 Trial Accuracy:  0.5714285714285714\n",
      "3 Trial Accuracy:  0.5714285714285714\n",
      "4 Trial Accuracy:  0.42857142857142855\n",
      "5 Trial Accuracy:  0.35714285714285715\n",
      "6 Trial Accuracy:  0.2857142857142857\n",
      "7 Trial Accuracy:  0.5\n",
      "8 Trial Accuracy:  0.21428571428571427\n",
      "9 Trial Accuracy:  0.35714285714285715\n",
      "10 Trial Accuracy:  0.5\n",
      "\n",
      "\n",
      "Avg. Accuracy for SVC - Linear:  0.42142857142857143\n",
      "Confusion Matrix for SVC - Linear: \n",
      " [[ 0.  0.]\n",
      " [60. 80.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cv = cross_validation.KFold(len(X), n_folds=10)\n",
    "acc_res = []\n",
    "cm_totallr= np.zeros((2,2))\n",
    "\n",
    "\n",
    "print(\"\\n Logistic Regression Accuracy\")\n",
    "for i, (train_ts, test_ts) in enumerate(cv):\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X[train_ts],y[train_ts])\n",
    "    y_predclf = clf.predict(X[test_ts])\n",
    "    print(i+1 ,\"Trial Accuracy: \",accuracy_score(y_predclf, y[test_ts]))\n",
    "    acc_res.append(accuracy_score(y_predclf, y[test_ts]))\n",
    "    cm_lr = confusion_matrix(y[test_ts], y_predclf)\n",
    "    cm_totallr += cm_lr\n",
    "print(\"\\n\")\n",
    "print(\"Avg. Accuracy for Logistic Regression: \",np.mean(acc_res))\n",
    "print(\"Confusion Matrix for Logistic Regression: \\n\",cm_totallr)\n",
    "print(\"\\n Bernoulli Naive Bayes Accuracy\")\n",
    "\n",
    "cv = cross_validation.KFold(len(X), n_folds=10)\n",
    "acc_resnb = []\n",
    "#Naive Bayes Classifier\n",
    "cm_totalbnb= np.zeros((2,2))\n",
    "for i, (train_ts, test_ts) in enumerate(cv):\n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(X[train_ts], y[train_ts])\n",
    "    y_predbnb = bnb.predict(X[test_ts])\n",
    "    print(i+1 ,\"Trial Accuracy: \",accuracy_score(y_predbnb,  y[test_ts]))\n",
    "    acc_resnb.append(accuracy_score(y_predbnb, y[test_ts]))\n",
    "    cm_bnb = confusion_matrix(y[test_ts], y_predbnb)\n",
    "    cm_totalbnb += cm_bnb\n",
    "print(\"\\n\")\n",
    "print(\"Avg. Accuracy for Bernoulli Naive Bayes: \",np.mean(acc_resnb))\n",
    "print(\"Confusion Matrix for Bernoulli Naive Bayes: \\n\",cm_totalbnb)\n",
    "\n",
    "cv = cross_validation.KFold(len(X), n_folds=10)\n",
    "print(\"\\n SVC - Linear Accuracy\")\n",
    "acc_reslin = []\n",
    "#SVM Classifier\n",
    "cm_totallin= np.zeros((2,2))\n",
    "for i, (train_ts, test_ts) in enumerate(cv):\n",
    "    svclin = svm.SVC(kernel=\"linear\", C=1.0)\n",
    "    svclin.fit(X[train_ts], y[train_ts])\n",
    "    y_predlin = svclin.predict(X[test_ts])\n",
    "    print(i+1 ,\"Trial Accuracy: \",accuracy_score(y_predlin,  y[test_ts]))\n",
    "    acc_reslin.append(accuracy_score(y_predlin, y[test_ts]))\n",
    "    cm_lin = confusion_matrix(y[test_ts], y_predlin)\n",
    "    cm_totallin += cm_bnb\n",
    "print(\"\\n\")\n",
    "print(\"Avg. Accuracy for SVC - Linear: \",np.mean(acc_reslin))\n",
    "print(\"Confusion Matrix for SVC - Linear: \\n\",cm_totallin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression\n",
      "Scores: \n",
      "Precision: 0.5\n",
      "Recall: 0.25\n",
      "Fscore: 0.3333333333333333\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Bernoulli Naive Bayes\n",
      "Scores: \n",
      "Precision: 0.5\n",
      "Recall: 0.2857142857142857\n",
      "Fscore: 0.36363636363636365\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "SVC\n",
      "Scores: \n",
      "Precision: 0.5\n",
      "Recall: 0.25\n",
      "Fscore: 0.3333333333333333\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nivea\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "lrscore =[]\n",
    "print(\"Logistic regression\")\n",
    "lrscore = precision_recall_fscore_support(y[test_ts], y_predclf)\n",
    "print(\"Scores: \")\n",
    "print(\"Precision:\",np.mean(lrscore[0]))\n",
    "print(\"Recall:\",np.mean(lrscore[1]))\n",
    "print(\"Fscore:\",np.mean(lrscore[2]))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"\")\n",
    "print(\"Bernoulli Naive Bayes\")\n",
    "bnbscore = precision_recall_fscore_support(y[test_ts], y_predbnb)\n",
    "print(\"Scores: \")\n",
    "print(\"Precision:\",np.mean(bnbscore[0]))\n",
    "print(\"Recall:\",np.mean(bnbscore[1]))\n",
    "print(\"Fscore:\",np.mean(bnbscore[2]))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"\")\n",
    "print(\"SVC\")\n",
    "svcscore = precision_recall_fscore_support(y[test_ts], y_predlin)\n",
    "print(\"Scores: \")\n",
    "print(\"Precision:\",np.mean(svcscore[0]))\n",
    "print(\"Recall:\",np.mean(svcscore[1]))\n",
    "print(\"Fscore:\",np.mean(svcscore[2]))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "SVC with rbf kernel:\n",
      "{'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.62      0.61        34\n",
      "          1       0.63      0.61      0.62        36\n",
      "\n",
      "avg / total       0.61      0.61      0.61        70\n",
      "\n",
      "[[21 13]\n",
      " [14 22]]\n",
      "-----------------------------------------------------\n",
      "Linear SVC:\n",
      "{'C': 1, 'loss': 'hinge'}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.59      0.60        34\n",
      "          1       0.62      0.64      0.63        36\n",
      "\n",
      "avg / total       0.61      0.61      0.61        70\n",
      "\n",
      "[[20 14]\n",
      " [13 23]]\n",
      "-----------------------------------------------------\n",
      "Bernoulli Naive Bayes:\n",
      "{'alpha': 1}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.76      0.72        34\n",
      "          1       0.75      0.67      0.71        36\n",
      "\n",
      "avg / total       0.72      0.71      0.71        70\n",
      "\n",
      "[[26  8]\n",
      " [12 24]]\n",
      "-----------------------------------------------------\n",
      "Logistic Regression:\n",
      "{'C': 10}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.59      0.62        34\n",
      "          1       0.65      0.72      0.68        36\n",
      "\n",
      "avg / total       0.66      0.66      0.66        70\n",
      "\n",
      "[[20 14]\n",
      " [10 26]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Grid search for best parameters\n",
    "\n",
    "# Explicitly specify default parameters fro each classifier for convenience\n",
    "svc_rbf = svm.SVC(kernel=\"rbf\", C=1.0, gamma=\"auto\")\n",
    "svc_lin = svm.LinearSVC(C=1.0, loss=\"squared_hinge\")\n",
    "bnb = BernoulliNB(alpha = 1.0)\n",
    "lr = LogisticRegression(C=1.0, penalty='l2')\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "\n",
    "# SVC with rbf kernel\n",
    "tuned_parameters_svc = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "clfgs = GridSearchCV(svc_rbf, tuned_parameters_svc, cv=5, scoring=\"recall\")\n",
    "clfgs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "\n",
    "print(\"SVC with rbf kernel:\")\n",
    "print(clfgs.best_params_)\n",
    "y_true, y_pred = y_test, clfgs.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "# Linear SVC\n",
    "tuned_parameters_lin_svc = [{'C': [1, 10, 100, 1000], \"loss\" : [\"squared_hinge\", \"hinge\"]}]\n",
    "\n",
    "clfgs = GridSearchCV(svc_lin, tuned_parameters_lin_svc, cv=5, scoring=\"recall\")\n",
    "clfgs.fit(X_train, y_train)\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "print(\"Linear SVC:\")\n",
    "print(clfgs.best_params_)\n",
    "y_true, y_pred = y_test, clfgs.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "\n",
    "# BerNB\n",
    "tuned_parameters_berNB = [{'alpha': [1, 10, 100, 1000]}]\n",
    "\n",
    "clfgs = GridSearchCV(bnb, tuned_parameters_berNB, cv=5, scoring=\"recall\")\n",
    "clfgs.fit(X_train, y_train)\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(clfgs.best_params_)\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "tuned_parameters_lr = [{\"C\" : [1, 10, 100, 1000]}]\n",
    "\n",
    "clfgs = GridSearchCV(lr, tuned_parameters_lr, cv=5, scoring=\"recall\")\n",
    "clfgs.fit(X_train, y_train)\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "print(clfgs.best_params_)\n",
    "y_true, y_pred = y_test, clfgs.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Avg. accuracy:  0.47857142857142854\n",
      "\n",
      "Agg. contingency table\n",
      "[[37. 33.]\n",
      " [40. 30.]]\n",
      "\n",
      "Avg. contingency table\n",
      "[[3.7 3.3]\n",
      " [4.  3. ]]\n",
      "\n",
      "Scores: \n",
      "Precision: 0.5\n",
      "Recall: 0.25\n",
      "Fscore: 0.3333333333333333\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nivea\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Fitting logistic regression with best parameters found\n",
    "\n",
    "acc_reslr = []\n",
    "lr_cm = np.zeros(shape = (2,2) )\n",
    "\n",
    "cv = cross_validation.KFold(len(X), n_folds=10)\n",
    "\n",
    "# Fit logistic regression with the largest magnitude coeefcient, which is weight\n",
    "for ix, (traincv, testcv) in enumerate(cv):\n",
    "    # Create new logistic regression instance\n",
    "    classifier = LogisticRegression(C=1.0)\n",
    "    # Fit the model\n",
    "    classifier.fit(X[traincv], y[traincv])\n",
    "    y_pred = classifier.predict(X[testcv])\n",
    "       \n",
    "    acc_reslr.append(accuracy_score(y_pred, y[testcv]))\n",
    "    cm = confusion_matrix(y[testcv], y_pred)\n",
    "    lr_cm += cm\n",
    "    \n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Avg. accuracy: \",np.mean(acc_reslr))\n",
    "\n",
    "print(\"\\nAgg. contingency table\")    \n",
    "print(lr_cm)\n",
    "print(\"\\nAvg. contingency table\")    \n",
    "print(lr_cm/10)\n",
    "lrscore = precision_recall_fscore_support(y[testcv], y_pred)\n",
    "print(\"\\nScores: \")\n",
    "print(\"Precision:\",np.mean(lrscore[0]))\n",
    "print(\"Recall:\",np.mean(lrscore[1]))\n",
    "print(\"Fscore:\",np.mean(lrscore[2]))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Avg. accuracy:  nan\n",
      "\n",
      "Agg. contingency table\n",
      "[[38. 32.]\n",
      " [35. 35.]]\n",
      "\n",
      "Avg. contingency table\n",
      "[[3.8 3.2]\n",
      " [3.5 3.5]]\n",
      "\n",
      "Scores: \n",
      "Precision: 0.5\n",
      "Recall: 0.2857142857142857\n",
      "Fscore: 0.36363636363636365\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nivea\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Nivea\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\Nivea\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Fitting Naive Bayes with best parameters found\n",
    "\n",
    "acc_resbnb = []\n",
    "bnb_cm = np.zeros(shape = (2,2) )\n",
    "\n",
    "cv = cross_validation.KFold(len(X), n_folds=10)\n",
    "\n",
    "# Fit logistic regression with the largest magnitude coeefcient, which is weight\n",
    "for ix, (traincv, testcv) in enumerate(cv):\n",
    "    # Create new logistic regression instance\n",
    "    classifier = BernoulliNB(alpha=1)\n",
    "    # Fit the model\n",
    "    classifier.fit(X[traincv], y[traincv])\n",
    "    y_pred = classifier.predict(X[testcv])\n",
    "       \n",
    "    acc_reslr.append(accuracy_score(y_pred, y[testcv]))\n",
    "    cm = confusion_matrix(y[testcv], y_pred)\n",
    "    bnb_cm += cm\n",
    "    \n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Avg. accuracy: \",np.mean(acc_resbnb))\n",
    "\n",
    "print(\"\\nAgg. contingency table\")    \n",
    "print(bnb_cm)\n",
    "print(\"\\nAvg. contingency table\")    \n",
    "print(bnb_cm/10)\n",
    "bnbscore = precision_recall_fscore_support(y[testcv], y_pred)\n",
    "print(\"\\nScores: \")\n",
    "print(\"Precision:\",np.mean(bnbscore[0]))\n",
    "print(\"Recall:\",np.mean(bnbscore[1]))\n",
    "print(\"Fscore:\",np.mean(bnbscore[2]))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Avg. accuracy:  0.5142857142857142\n",
      "\n",
      "Agg. contingency table\n",
      "[[41. 29.]\n",
      " [39. 31.]]\n",
      "\n",
      "Avg. contingency table\n",
      "[[4.1 2.9]\n",
      " [3.9 3.1]]\n",
      "\n",
      "Scores: \n",
      "Precision: 0.5\n",
      "Recall: 0.32142857142857145\n",
      "Fscore: 0.391304347826087\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nivea\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Fitting SVM linear with best parameters found\n",
    "\n",
    "acc_reslin = []\n",
    "lin_cm = np.zeros(shape = (2,2) )\n",
    "\n",
    "cv = cross_validation.KFold(len(X), n_folds=10)\n",
    "\n",
    "# Fit logistic regression with the largest magnitude coeefcient, which is weight\n",
    "for ix, (traincv, testcv) in enumerate(cv):\n",
    "    # Create new logistic regression instance\n",
    "    classifier = svm.LinearSVC(C=1000, loss=\"hinge\")\n",
    "    # Fit the model\n",
    "    classifier.fit(X[traincv], y[traincv])\n",
    "    y_pred = classifier.predict(X[testcv])\n",
    "       \n",
    "    acc_reslin.append(accuracy_score(y_pred, y[testcv]))\n",
    "    cm = confusion_matrix(y[testcv], y_pred)\n",
    "    lin_cm += cm\n",
    "    \n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Avg. accuracy: \",np.mean(acc_reslin))\n",
    "\n",
    "print(\"\\nAgg. contingency table\")    \n",
    "print(lin_cm)\n",
    "print(\"\\nAvg. contingency table\")    \n",
    "print(lin_cm/10)\n",
    "linscore = precision_recall_fscore_support(y[testcv], y_pred)\n",
    "print(\"\\nScores: \")\n",
    "print(\"Precision:\",np.mean(linscore[0]))\n",
    "print(\"Recall:\",np.mean(linscore[1]))\n",
    "print(\"Fscore:\",np.mean(linscore[2]))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEjCAYAAADdZh27AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+eRPECvdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c24bb72208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEjCAYAAADdZh27AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAH79JREFUeJzt3XucVXW9//HXGxTRwBsQYwIOGpbk\nhXQktU7S0RQ1r8cEspLfMTlq5vFSP/XYr+Px5KMedrFjaonmD+0keM3QKEqPt5+CijcUzESkmAAl\nvIQmCvr5/bHWLLfDzJ41l7X37D3v5+OxH+611net9VkzuN+zLvv7VURgZmYG0K/aBZiZWe/hUDAz\ns4xDwczMMg4FMzPLOBTMzCzjUDAzs4xDwczMMg4Fq0mSlkl6U9LrklZJmiFpUKs2+0n6H0lrJb0m\n6XZJY1u12VLSjyT9Od3WknR6aDv7laTTJT0t6Q1JzZJukrRbkcdrVikOBatlh0fEIGAc8HHgvJYF\nkvYFfgf8CvgQMBp4EnhA0o5pmwHAXcDHgInAlsB+wBpgfDv7/C/gX4HTgW2BnYHbgMM6W7ykTTq7\njlnR5G80Wy2StAz4SkTcmU5fDHwsIg5Lp+8HnoqIU1ut9xtgdUR8WdJXgIuAnSLi9Rz7HAP8Adg3\nIh5up809wH9HxNXp9NS0zk+l0wGcBpwBbALMBV6PiK+XbONXwL0R8UNJHwJ+DHwaeB24JCIuzfEj\nMusSnylYzZM0AjgEWJJOb0HyF/9NbTS/Efhs+v5A4Ld5AiF1ANDcXiB0wlHAJ4CxwPXAJEkCkLQN\ncBAwS1I/4HaSM5zt0/2fIengbu7frF0OBatlt0laCywHXgL+PZ2/Lcm/7ZVtrLMSaLlfMKSdNu3p\nbPv2fCciXo6IN4H7gQD+IV12LDAvIlYAewPDIuLCiHg7IpYCVwGTe6AGszY5FKyWHRURg4EJwEd5\n78P+FeBdYLs21tkO+Gv6fk07bdrT2fbtWd7yJpLrt7OAKemsLwC/SN/vAHxI0qstL+DfgOE9UINZ\nmxwKVvMi4l5gBvD9dPoNYB7w+TaaH0dycxngTuBgSR/Iuau7gBGSmsq0eQPYomS6oa2SW03PBI6V\ntAPJZaVb0vnLgRciYuuS1+CIODRnvWad5lCwevEj4LOSxqXT5wInpI+PDpa0jaRvA/sC/5G2+TnJ\nB+8tkj4qqZ+kIZL+TdJGH7wR8RxwBTBT0gRJAyQNlDRZ0rlpsyeAYyRtIenDwIkdFR4RjwOrgauB\nuRHxarroYeBvks6RtLmk/pJ2lbR3V35AZnk4FKwuRMRq4Drg/6TT/w84GDiG5D7An0geW/1U+uFO\nRLxFcrP5D8Dvgb+RfBAPBR5qZ1enA5cBlwOvAs8DR5PcEAa4BHgbeBG4lvcuBXVkZlrL9SXH9A5w\nOMkjty+QXPa6Gtgq5zbNOs2PpJqZWcZnCmZmlnEomJlZxqFgZmYZh4KZmWUcCmZmlnEomJlZxqFg\nZmYZh4KZmWUcCmZmlnEomJlZxqFgZmYZh4KZmWUcCmZmlnEomJlZxqFgZmYZh4KZmWUcCmZmltmk\n2gV01tChQ6OxsbHaZZiZ1ZRHH330rxExrKN2NRcKjY2NLFiwoNplmJnVFEl/ytPOl4/MzCzjUDAz\ns4xDwczMMg4FMzPLOBTMzCxTWChIukbSS5Kebme5JF0qaYmkhZL2LKoWM7Oa1dAA0savhoZCdlfk\nmcIMYGKZ5YcAY9LXNOAnBdZiZlabXnyxc/O7qbBQiIj7gJfLNDkSuC4S84GtJW1XVD1mZtaxat5T\n2B5YXjLdnM7biKRpkhZIWrB69eqKFGdm1hdVMxTUxrxoq2FETI+IpohoGjasw29pm5lZF1UzFJqB\nkSXTI4AVVarFzMyobijMBr6cPoW0D/BaRKysYj1mZr3P8OGdm99NRT6SOhOYB3xEUrOkEyWdLOnk\ntMkcYCmwBLgKOLWoWszMataqVbAy/Xv5Jz+BiOS1alUhuyusl9SImNLB8gC+WtT+zcys8/yNZjMz\nyzgUzMws41Aws76pwt1HdFlDA2yXfq/3lFMKr7PmRl4zM+sR5bqPWNGLno6vcDcXDgUzs9a2b7Nz\nhT7BoWBm1tqVV1a7gvf8y79UdHcOBTOz1qZNq3YF76lwKPhGs5mZZRwKZtY3Vbj7iC6rl24uzMx6\ntVWrku4ipk1LHu8suPuILmups/WroDodCmZmlnEomJlZxqFgZmYZh4KZmWUcCmZmlnEomJlZxqFg\nZmYZh4KZmWUcCmZmlnEomJlZxqFgZmYZh4KZmWUcCmYtamXM3npV6Z9/y/6mT086l/PvG3AomL2n\nwmPhWiuV/vn7990mj7xmlsevf13tCvo2//wrxqFglsfnPlftCvo2//wrxqFglsfDD1e7gvo3fnz7\ny4r4+ZfbXx/mUDDLY++9q11B3+aff8X4RrNZi1oZs7deVfrn7993mxwKZi1WrYLly5P3V13Ve8fs\nrVcVHou44vurEQ4FMzPLOBTMzCxTaChImijpWUlLJJ3bxvJRku6W9LikhZIOLbIeMzMrr7BQkNQf\nuBw4BBgLTJE0tlWzbwI3RsTHgcnAFUXVYzWoGt0ejByZvD/pJHd7YH1SkWcK44ElEbE0It4GZgFH\ntmoTwJbp+62AFQXWY7XG3R6YVVyR31PYHlheMt0MfKJVmwuA30n6GvAB4MAC67F6MmJEtSswq0tF\nhoLamBetpqcAMyLiB5L2BX4uadeIePd9G5KmAdMARo0aVUixVmMmTuz5bf7sZz2/TbMaU2QoNAMj\nS6ZHsPHloROBiQARMU/SQGAo8FJpo4iYDkwHaGpqah0s1hddfXXPb9OhYFboPYVHgDGSRksaQHIj\neXarNn8GDgCQtAswEFhdYE1mZlZGYaEQERuA04C5wDMkTxktknShpCPSZmcDJ0l6EpgJTI0InwlY\nwt0emFWcau0zuKmpKRYsWFDtMqyS9t0XttwS5s6tdiVmNUvSoxHR1FE7f6PZzMwyDgUzM8s4FMzM\nLONQMDOzjEPBzMwyDgUzM8s4FMzMLONQMDOzTK5QkDRA0oeLLsbMzKqrw1CQdBjwFPD7dHqcpF8W\nXZiZmVVenjOFC0nGQXgVICKeAHzWYGZWh/KEwvqIeLXVvNrqMMnMzHLJM57CM5KOA/pJGg38KzC/\n2LLMzKwa8oTCacC3gHeBW0m6wj6vyKLMAGhoeP/4yEoH8xs+HFatqk5NZnUuTygcHBHnAOe0zJB0\nDElAmBWnNBDyzDezbstzT+Gbbcw7v6cLMTOz6mv3TEHSwSTjJ28v6Ycli7YkuZRkZmZ1ptzlo5eA\np4F1wKKS+WuBc4ssyszMqqPdUIiIx4HHJf0iItZVsCYzM6uSPPcUtpc0S9JCSX9seRVemdnw4Z2b\nb2bdlicUZgD/FxBwCHAjMKvAmswSq1ZBBOyzDxx0UPI+wo+jmhUoTyhsERFzASLi+Yj4JvCZYssy\nM7NqyPM9hbckCXhe0snAX4APFluWmZlVQ55QOBMYBJwOXARsBfxzkUWZmVl1dBgKEfFQ+nYt8CUA\nSSOKLMrMzKqj7D0FSXtLOkrS0HT6Y5Kuwx3imZnVpXZDQdJ3gF8AxwO/lXQ+cDfwJLBzZcozM7NK\nKnf56Ehgj4h4U9K2wIp0+tnKlGZmZpVW7vLRuoh4EyAiXgb+4EAwM6tv5c4UdpTU0j22gMaSaSLi\nmEIrMzOziisXCv/UavqyIgsxM7PqK9ch3l2VLMTMzKovTzcXZmbWRxQaCpImSnpW0hJJbY7BIOk4\nSYslLZJ0fZH1WDc1NCTjJLd+NTQUu7/58+F3vyt+f2aWq5sLACRtFhFvdaJ9f+By4LNAM/CIpNkR\nsbikzRjgPOCTEfGKJPep1JtVesxkj9FsVnEdhoKk8cDPSPo8GiVpD+ArEfG1DlYdDyyJiKXpdmaR\nfPdhcUmbk4DLI+IVgIh4qfOHYL3C3LnVrsDMekCeM4VLgc8BtwFExJOS8nSdvT2wvGS6GfhEqzY7\nA0h6AOgPXBARv229IUnTgGkAo0aNyrFrq7iJE6tdgZn1gDyh0C8i/pT0np15J8d6amNetLH/McAE\nYARwv6RdI+LV960UMR2YDtDU1NR6G9YbPPhgz29zv/16fptmVlaeUFieXkKK9D7B14A8w3E2AyNL\npkeQdJXRus38iFgPvCDpWZKQeCTH9q032XffaldgZj0gz9NHpwBnAaOAF4F90nkdeQQYI2m0pAHA\nZGB2qza3kY7ilvbEujOwNF/pVnGVHjPZYzSbVVyeUNgQEZMjYmj6mhwRf+1opYjYAJwGzAWeAW6M\niEWSLpR0RNpsLrBG0mKSHli/ERFrungsVrSWMZO///1k+m9/K3bM5Jb9tX55jGazwuS5fPRIelnn\nBuDWiFibd+MRMQeY02ret0reB8lZyFl5t2lmZsXp8EwhInYCvg3sBTwl6TZJkwuvzMzMKi7XN5oj\n4sGIOB3YE/gbyeA7ZmZWZzoMBUmDJB0v6XbgYWA14GcFzczqUJ57Ck8DtwMXR8T9BddjZmZVlCcU\ndoyIdwuvxMzMqq7dUJD0g4g4G7hF0kbfIvbIa2Zm9afcmcIN6X894pqZWR9RbuS1h9O3u0TE+4JB\n0mmAR2YzM6szeR5J/ec25p3Y04WYmVn1lbunMImkv6LRkm4tWTQYeLXttczMrJaVu6fwMLCGpHfT\ny0vmrwUeL7IoMzOrjnL3FF4AXgDurFw5ZmZWTeUuH90bEftLeoX3D44jkr7sti28OitGQ0Pb4xwP\nH16+B9LW6225Zb71zKxmlLt81DLk5tBKFGIV1N7A9+3N7+56ZlYzyl0+avkW80hgRUS8LelTwO7A\nf5N0jGf15rvfrXYFZlZFebq5uA3YW9JOwHXAr4Hrgc8VWZhVyXnnVbsCM6uiPKHwbkSsl3QM8KOI\nuFSSnz6qVbFRjyXv9+ab7S/bfPOercXMep08obBB0ueBLwFHpfM2La4kK8yrr8KJHXzvcODAytRi\nZr1S3m80f4ak6+ylkkYDM4sty3rco4/CXnvB7NkwaFDbbYYPL7+N9pZ3tJ6Z1Yw8w3E+DZwOLJD0\nUWB5RFxUeGXWMyLgsstgv/1g/Xq47z5YuzaZ3/rV0WOlq1Z1bT0zqxkdXj6S9A/Az4G/kHxHoUHS\nlyLigaKLs2567TU46SS46SY47DC49loYMqTaVZlZL5bnnsIlwKERsRhA0i4kIdFUZGHWTY89Bscd\nB8uWwcUXw9lnQ79cQ3KbWR+W51NiQEsgAETEM8CA4kqybomAK66AffeFt95KLhd94xsOBDPLJc+Z\nwmOSriQ5OwA4HneI176udiHR0/sbMAAefxyG+gvpZpZfnlA4meRG8/8muadwH/DjIouqaeW6gni1\ngB7H29vf2287EMys08qGgqTdgJ2AX0bExZUpqY5ts021KzAzK6tcL6n/RjLC2mMk3VxcGBHXVKyy\nenTJJT2/zTPP7PltmlmfpWin2wNJi4DxEfGGpGHAnIjYu6LVtaGpqSkWLFhQ7TLaJ7W/rKMuJmph\nf2ZWkyQ9GhEdPjVa7pGUtyLiDYCIWN1BWzMzqwPl7insWDI2s4CdSsdqjohjCq2sVg0f3v7TR/Ww\nPzOra+VC4Z9aTV9WZCF1o+Wx0/Hjk6d/5sypzP7MzHpAuUF27qpkIWZmVn2+T2BmZplCQ0HSREnP\nSloi6dwy7Y6VFJLcn5KZWRXlDgVJm3Vmw5L6A5cDhwBjgSmSxrbRbjDJN6Yf6sz2zcys53UYCpLG\nS3oKeC6d3kNSnm4uxgNLImJpRLwNzAKObKPdfwIXA+vyl21mZkXIc6ZwKfA5YA1ARDxJMhJbR7YH\nlpdMN6fzMpI+DoyMiDvKbUjSNEkLJC1YvXp1jl2bmVlX5AmFfhHxp1bz3smxXltftc2+YiupH8lY\nDWd3tKGImB4RTRHRNGzYsBy7NjOzrsgTCssljQdCUn9JZwB/zLFeMzCyZHoEsKJkejCwK3CPpGXA\nPsBs32w2M6uePKFwCnAWMAp4keTD+5Qc6z0CjJE0WtIAYDIwu2VhRLwWEUMjojEiGoH5wBER0Ys7\nNjIzq28djqcQES+RfKB3SkRskHQaMBfoD1wTEYskXQgsiIjZ5bdgZmaV1mEoSLqKknsBLSJiWkfr\nRsQcYE6red9qp+2EjrZnZmbFyjPy2p0l7wcCR/P+p4rMzKxO5Ll8dEPptKSfA78vrCIzM6uarnRz\nMRrYoacLMTOz6stzT+EV3run0A94GWi3HyMzM6tdZUNBkoA9gL+ks96N9sbvNDOzmlf28lEaAL+M\niHfSlwPBzKyO5bmn8LCkPQuvxMzMqq7dy0eSNomIDcCngJMkPQ+8QdKnUUSEg8LMrM6Uu6fwMLAn\ncFSFajEzsyorFwoCiIjnK1SLmZlVWblQGCbprPYWRsQPC6jHzMyqqFwo9AcG0fa4CGZmVofKhcLK\niLiwYpX0Ng0N8OKLG88fPhxWrcq/npRvPTOzXqDcI6l9+wyhrUAoN7+765mZ9QLlzhQOqFgVtWbn\nnatdgZlZIdoNhYh4uZKF1JSmMiOGPvdc5eowM+thecZTsNauv779ZTNnVq4OM7Me1pWus83MrE45\nFNozfHjn5nd3PTOzXsCh0J5VqyACpk6FUaOS9xEdP1basl7rlx9HNbMa4FAwM7OMQ8HMzDIOBTMz\nyzgUzMws41AwM7OMQ8HMzDIOBTMzyzgUzMws41AwM7OMQ8HMzDIOBTMzyzgUzMwsU2goSJoo6VlJ\nSySd28bysyQtlrRQ0l2SdiiyHjMzK6+wUJDUH7gcOAQYC0yRNLZVs8eBpojYHbgZuLioeszMrGNF\nnimMB5ZExNKIeBuYBRxZ2iAi7o6Iv6eT84ERBdZjZmYdKDIUtgeWl0w3p/PacyLwmwLrMTOzDhQ5\nRrPamBdtNpS+CDQB+7ezfBowDWDUqFE9VZ+ZmbVS5JlCMzCyZHoEsKJ1I0kHAucDR0TEW21tKCKm\nR0RTRDQNGzaskGLNzKzYUHgEGCNptKQBwGRgdmkDSR8HriQJhJcKrMXMzHIoLBQiYgNwGjAXeAa4\nMSIWSbpQ0hFps+8Bg4CbJD0haXY7mzMzswoo8p4CETEHmNNq3rdK3h9Y5P7NzKxz/I1mMzPLOBTM\nzCzjUDAzs4xDwczMMg4FMzPLOBTMzCzjUDAzs4xDwczMMg4FMzPLOBTMzCzjUDAzs4xDwczMMg4F\nMzPLFNpLaq/Q0AAvvrjx/OHDYdWq/OtJ+dYzs15r/fr1NDc3s27dumqXUpiBAwcyYsQINt100y6t\nX/+h0FYgtMy///6urWdmNam5uZnBgwfT2NiI1NaIwbUtIlizZg3Nzc2MHj26S9uo/1Ao59OfrnYF\nZlZB69atq9tAAJDEkCFDWL16dZe30bdD4c472192oMf/MatH9RoILbp7fH07FA44oNoVmFkfdNFF\nF3H99dfTv39/+vXrx3bbbce4ceP4zne+k7V54oknmDJlCs888wyNjY2MHDmS+0sueY8bN44NGzbw\n9NNP92htfvrIzKwtDQ3JAyatXw0N3drsvHnzuOOOO3jsscdYuHAhd955J+eeey433HDD+9rNmjWL\nL3zhC9n02rVrWb58OQDPPPNMt2oop/5DYfjwzs3v7npmVh8Kethk5cqVDB06lM022wyAoUOHsv/+\n+7P11lvz0EMPZe1uvPFGJk+enE0fd9xxWXDMnDmTKVOmdKuO9tR/KKxaBRFw8cXJ9OuvJ9MdPVba\nsl7rlx9HNasPZ5wBEya0/yqnvXXOOKPD3R500EEsX76cnXfemVNPPZV7770XgClTpjBr1iwA5s+f\nz5AhQxgzZky23rHHHsutt94KwO23387hhx+e/1g7of5DwcysFxk0aBCPPvoo06dPZ9iwYUyaNIkZ\nM2YwefJkbr75Zt59911mzZq10ZnAtttuyzbbbMOsWbPYZZdd2GKLLQqpr2/faDazvutHPyq/vNxT\nPPfc061d9+/fnwkTJjBhwgR22203rr32WqZOnUpjYyP33nsvt9xyC/PmzdtovUmTJvHVr36VGTNm\ndGv/5dR/KLT+ZvKgQcl//c1kM6uCZ599ln79+mWXhp544gl22GEHILmEdOaZZ7LTTjsxYsSIjdY9\n+uijWblyJQcffDArVqwopL76v3zkbyabWVcU9LDJ66+/zgknnMDYsWPZfffdWbx4MRdccAEAn//8\n51m0aNH7bjCXGjx4MOeccw4DBgzoVg3l1P+ZgplZVxR0JWGvvfbiwQcfbHPZsGHDWL9+/Ubzly1b\nttG8xsbGHv+OAvSFMwUzM8vNoWBmZhmHgpmZZeo/FPzNZDMrERHVLqFQ3T2++r/R7MdOzSw1cOBA\n1qxZw5AhQ+qyt9SW8RQGDhzY5W3UfyiYmaVGjBhBc3Nzt8Yb6O1aRl7rKoeCmfUZm266aZdHJOsr\n6v+egpmZ5eZQMDOzjEPBzMwyqrXHsyStBv7UxdWHAn/twXJqgY+5b/Ax9w3dOeYdImJYR41qLhS6\nQ9KCiGiqdh2V5GPuG3zMfUMljtmXj8zMLONQMDOzTF8LhenVLqAKfMx9g4+5byj8mPvUPQUzMyuv\nr50pmJlZGXUZCpImSnpW0hJJ57axfDNJN6TLH5LUWPkqe1aOYz5L0mJJCyXdJWmHatTZkzo65pJ2\nx0oKSTX/pEqeY5Z0XPq7XiTp+krX2NNy/NseJeluSY+n/74PrUadPUXSNZJektTmsGpKXJr+PBZK\n2rNHC4iIunoB/YHngR2BAcCTwNhWbU4Ffpq+nwzcUO26K3DMnwG2SN+f0heOOW03GLgPmA80Vbvu\nCvyexwCPA9uk0x+sdt0VOObpwCnp+7HAsmrX3c1j/jSwJ/B0O8sPBX4DCNgHeKgn91+PZwrjgSUR\nsTQi3gZmAUe2anMkcG36/mbgANV2P7odHnNE3B0Rf08n5wNd70axd8jzewb4T+BiYF0liytInmM+\nCbg8Il4BiIiXKlxjT8tzzAFsmb7fClhRwfp6XETcB7xcpsmRwHWRmA9sLWm7ntp/PYbC9sDykunm\ndF6bbSJiA/AaMKQi1RUjzzGXOpHkL41a1uExS/o4MDIi7qhkYQXK83veGdhZ0gOS5kuaWLHqipHn\nmC8AviipGZgDfK0ypVVNZ/9/75R67Dq7rb/4Wz9iladNLcl9PJK+CDQB+xdaUfHKHrOkfsAlwNRK\nFVQBeX7Pm5BcQppAcjZ4v6RdI+LVgmsrSp5jngLMiIgfSNoX+Hl6zO8WX15VFPr5VY9nCs3AyJLp\nEWx8Opm1kbQJySlnudO13i7PMSPpQOB84IiIeKtCtRWlo2MeDOwK3CNpGcm119k1frM577/tX0XE\n+oh4AXiWJCRqVZ5jPhG4ESAi5gEDSfoIqle5/n/vqnoMhUeAMZJGSxpAciN5dqs2s4ET0vfHAv8T\n6R2cGtXhMaeXUq4kCYRav84MHRxzRLwWEUMjojEiGknuoxwREQuqU26PyPNv+zaShwqQNJTkctLS\nilbZs/Ic85+BAwAk7UISCvU7tFpy/F9On0LaB3gtIlb21Mbr7vJRRGyQdBowl+TJhWsiYpGkC4EF\nETEb+BnJKeYSkjOEydWruPtyHvP3gEHATek99T9HxBFVK7qbch5zXcl5zHOBgyQtBt4BvhERa6pX\ndffkPOazgasknUlyGWVqLf+RJ2kmyeW/oel9kn8HNgWIiJ+S3Dc5FFgC/B34Xz26/xr+2ZmZWQ+r\nx8tHZmbWRQ4FMzPLOBTMzCzjUDAzs4xDwczMMg4F63UkvSPpiZJXY5m2je31JtnJfd6T9sT5ZNpF\nxEe6sI2TJX05fT9V0odKll0taWwP1/mIpHE51jlD0hbd3bf1DQ4F643ejIhxJa9lFdrv8RGxB0ln\nid/r7MoR8dOIuC6dnAp8qGTZVyJicY9U+V6dV5CvzjMAh4Ll4lCwmpCeEdwv6bH0tV8bbT4m6eH0\n7GKhpDHp/C+WzL9SUv8Odncf8OF03QPSfvqfSvu53yyd/129Nz7F99N5F0j6uqRjSfqX+kW6z83T\nv/CbJJ0i6eKSmqdK+nEX65xHSUdokn4iaYGScRT+I513Okk43S3p7nTeQZLmpT/HmyQN6mA/1oc4\nFKw32rzk0tEv03kvAZ+NiD2BScClbax3MvBfETGO5EO5Oe32YBLwyXT+O8DxHez/cOApSQOBGcCk\niNiNpAeAUyRtCxwNfCwidge+XbpyRNwMLCD5i35cRLxZsvhm4JiS6UnADV2scyJJtxYtzo+IJmB3\nYH9Ju0fEpST94nwmIj6Tdn3xTeDA9Ge5ADirg/1YH1J33VxYXXgz/WAstSlwWXoN/R2SPn1amwec\nL2kEcGtEPCfpAGAv4JG0e4/NSQKmLb+Q9CawjKT75Y8AL0TEH9Pl1wJfBS4jGZ/hakm/BnJ3zR0R\nqyUtTfuseS7dxwPpdjtT5wdIun0oHXXrOEnTSP6/3o5kwJmFrdbdJ53/QLqfASQ/NzPAoWC140zg\nRWAPkjPcjQbNiYjrJT0EHAbMlfQVkm6Gr42I83Ls4/jSDvMktTnGRtofz3iSTtgmA6cB/9iJY7kB\nOA74A/DLiAgln9C56yQZgey7wOXAMZJGA18H9o6IVyTNIOkYrjUBv4+IKZ2o1/oQXz6yWrEVsDLt\nI/9LJH8lv4+kHYGl6SWT2SSXUe4CjpX0wbTNtso/PvUfgEZJH06nvwTcm16D3yoi5pDcxG3rCaC1\nJN13t+VW4CiScQBuSOd1qs6IWE9yGWif9NLTlsAbwGuShgOHtFPLfOCTLcckaQtJbZ11WR/lULBa\ncQVwgqT5JJeO3mijzSTgaUlPAB8lGbJwMcmH5+8kLQR+T3JppUMRsY6kB8qbJD0FvAv8lOQD9o50\ne/eSnMW0NgP4acuN5lbbfQVYDOwQEQ+n8zpdZ3qv4gfA1yPiSZKxmRcB15BckmoxHfiNpLsjYjXJ\nk1Ez0/3MJ/lZmQHuJdXMzEr4TMHMzDIOBTMzyzgUzMws41AwM7OMQ8HMzDIOBTMzyzgUzMws41Aw\nM7PM/wcCrdUZuoxE1wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c24bcf9860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEjCAYAAADdZh27AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c24be25d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ROC Curves\n",
    "\n",
    "from sklearn import svm, datasets, metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "svc_rbf = svm.SVC(C = 100, gamma = 0.001, kernel = 'rbf', probability=True)\n",
    "bnb = BernoulliNB(alpha = 1.0)\n",
    "lr = LogisticRegression(C=10)\n",
    "\n",
    "#Splitting data into test and training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "y_score = lr.fit(X_train, y_train).predict_proba(X_test)[:, 1]\n",
    "\n",
    "false_positive, true_positive, thresholds = metrics.roc_curve(y_test, y_score)\n",
    "# Notice we are only using the True Y label, Not the Predicted Y label\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('ROC Curve')\n",
    "graph = fig.add_subplot(111)\n",
    "graph.plot(false_positive, true_positive, c='blue', marker=\"s\", label=\"SVM\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "#Bernoulli \n",
    "y_score = bnb.fit(X_train, y_train).predict_proba(X_test)[:, 1]\n",
    "\n",
    "false_positive, true_positive, thresholds = metrics.roc_curve(y_test, y_score)\n",
    "# Notice we are only using the True Y label, Not the Predicted Y label\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('ROC Curve')\n",
    "graph = fig.add_subplot(111)\n",
    "graph.plot(false_positive, true_positive, c='red', marker=\"s\", label=\"SVM\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Bernoulli \n",
    "y_score = svc_rbf.fit(X_train, y_train).predict_proba(X_test)[:, 1]\n",
    "\n",
    "false_positive, true_positive, thresholds = metrics.roc_curve(y_test, y_score)\n",
    "# Notice we are only using the True Y label, Not the Predicted Y label\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('ROC Curve')\n",
    "graph = fig.add_subplot(111)\n",
    "graph.plot(false_positive, true_positive, c='green', marker=\"s\", label=\"SVM\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
